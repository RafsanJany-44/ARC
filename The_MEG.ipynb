{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RafsanJany-44/ARC/blob/master/The_MEG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdFOS9nFlpsU",
        "outputId": "9da0c06c-ef21-4a1d-f30d-170a2dc9447c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sBgjkkgqNXAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEsFaHeE2Pss"
      },
      "source": [
        "###Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESoueLt11P_w"
      },
      "outputs": [],
      "source": [
        "!pip install imbalanced-learn\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "def balance(X_temp, y_temp):\n",
        "  smote = SMOTE()\n",
        "  X_temp, y_temp= smote.fit_resample(X_temp, y_temp)\n",
        "  return pd.concat([pd.DataFrame(X_temp), pd.DataFrame(y_temp)], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "def models_check_box(models):\n",
        "  import ipywidgets as widgets\n",
        "  from IPython.display import display\n",
        "  new_keys=[]\n",
        "  for i in models:\n",
        "    i=widgets.Checkbox(\n",
        "      value=False,\n",
        "      description=str(i),\n",
        "      disabled=False,\n",
        "      indent=False\n",
        "      )\n",
        "    display(i)\n",
        "    new_keys.append(i)\n",
        "  return new_keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1B_8Nsw9-1Q"
      },
      "source": [
        "#Starting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0AmIUwSmLfj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "#dataset=pd.read_csv(\"/content/drive/MyDrive/Sleep Stage XAI/EEG_HMC_FeatureExtraction_2023.01.19.csv\")\n",
        "#dataset=pd.read_excel(\"/content/EEG_CNU_Control_Resting, walking, working and Reading_2023.01.07.xlsx\")\n",
        "\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/Iqram Sir/EEG_HMC_FeatureExtraction_2023.01.19.csv\")\n",
        "\n",
        "target = \"Sleep Stage\"\n",
        "result = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YN_C8yxm1ho"
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SDC0qL01iJd"
      },
      "outputs": [],
      "source": [
        "dataset.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing NULL values"
      ],
      "metadata": {
        "id": "A3HDJubYkzut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.dropna(axis=0, how='any', inplace = True)\n",
        "dataset = dataset.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "oPb1X8ktk3yG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observing shape of dataset"
      ],
      "metadata": {
        "id": "ziEv61mYL9hK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "id": "3GZXJQDFk8sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "target value count"
      ],
      "metadata": {
        "id": "TuhzOMrgMDBY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NicZ_GzN1mTL"
      },
      "outputs": [],
      "source": [
        "dataset[target].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VS4iUbnyYaDH"
      },
      "outputs": [],
      "source": [
        "set(list(dataset[target]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Encoding"
      ],
      "metadata": {
        "id": "hulHTqVxMJky"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ljyj9yuIf-tX"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder=LabelEncoder()\n",
        "dataset[target]=encoder.fit_transform(dataset[target])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42si00q3SLh5"
      },
      "outputs": [],
      "source": [
        "set(list(dataset[target]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXMKr8bi2JnW"
      },
      "source": [
        "###Spliting into X and y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIf-rcIB11o1"
      },
      "outputs": [],
      "source": [
        "X =  dataset.loc[:,dataset.columns != target]  # removing Sleep Stage\n",
        "X =  X.loc[:,X.columns != \"Subject\"]            # removing Subject\n",
        "X =  X.loc[:,X.columns != \"Epoch\"]             # removing Epoch\n",
        "y = dataset[target]\n",
        "\n",
        "X.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G108WUM2Di_"
      },
      "source": [
        "###USing SMOTE for balancing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvJIm4Co2ApP"
      },
      "outputs": [],
      "source": [
        "new_dataset =  balance(X,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUzLAHJqACBH"
      },
      "outputs": [],
      "source": [
        "new_dataset[target].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZLedhGD91Cv"
      },
      "source": [
        "#Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1m6UgTv92XU"
      },
      "outputs": [],
      "source": [
        "number_of_feat = 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkC7pB1v-Gsp"
      },
      "source": [
        "###ANOVA with f classifciation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rafbj3rl-GWZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "fs = SelectKBest(score_func=f_classif, k=5)\n",
        "fit = fs.fit(X,y)\n",
        "dfscores = pd.DataFrame(fit.scores_)\n",
        "dfcolumns = pd.DataFrame(X.columns)\n",
        "\n",
        "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
        "\n",
        "featureScores.columns = ['Best_columns','Score_ANOVA'] \n",
        "\n",
        "lyst = featureScores.nlargest(number_of_feat,'Score_ANOVA')\n",
        "\n",
        "#lyst.to_csv('Filter_Method_ANOVA_with_f_classif.csv')\n",
        "\n",
        "list_of_feat = list(lyst[\"Best_columns\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCkHXMQQ-PoJ"
      },
      "source": [
        "###Embedded Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZJcRlL7-So6"
      },
      "outputs": [],
      "source": [
        "''''\n",
        "from sklearn.linear_model import LassoCV\n",
        "reg = LassoCV()\n",
        "reg.fit(X, y)\n",
        "print(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\n",
        "print(\"Best score using built-in LassoCV: %f\" %reg.score(X,y))\n",
        "coef = pd.Series(reg.coef_, index = X.columns)\n",
        "\n",
        "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
        "\n",
        "imp_coef = coef.sort_values()\n",
        "\n",
        "list_of_feat=[]\n",
        "\n",
        "\n",
        "for i in range(coef.shape[0]):\n",
        "  if coef[i]!=0:\n",
        "    list_of_feat.append(dataset.iloc[:0,i+3].name)\n",
        "    \n",
        "df = pd.DataFrame(list_of_feat, columns=['Best_Features'])\n",
        "\n",
        "#df.to_csv(\"Embedded_Method.csv\")\n",
        "\n",
        "list_of_feat = list(df[\"Best_Features\"])\n",
        "if number_of_feat < len(list_of_feat):\n",
        "  list_of_feat = list_of_feat[:number_of_feat]\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrR11h2K-aJn"
      },
      "source": [
        "###Pearson's with f regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH6yDZiP-a9K"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_regression\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "fs = SelectKBest(score_func=f_regression, k=5)\n",
        "fit = fs.fit(X,y)\n",
        "\n",
        "dfscores = pd.DataFrame(fit.scores_)\n",
        "dfcolumns = pd.DataFrame(dataset.columns)\n",
        "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
        "\n",
        "featureScores.columns = ['Best_columns','Score_pearsons'] \n",
        "\n",
        "\n",
        "lyst = featureScores.nlargest(number_of_feat,'Score_pearsons')\n",
        "\n",
        "#lyst.to_csv('Filter_Method_Pearsonâ€™s_with_f_regression.csv')\n",
        "\n",
        "list_of_feat = list(lyst[\"Best_columns\"])\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85wbxGFW-kGZ"
      },
      "source": [
        "###Sequential Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtP9AHew-k1i"
      },
      "outputs": [],
      "source": [
        "''''\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "sfs = SequentialFeatureSelector(knn, n_features_to_select=number_of_feat)\n",
        "sfs.fit(X, y)\n",
        "list_of_feat=[]\n",
        "list_of_feat=list(sfs.get_feature_names_out(X.columns))\n",
        "\n",
        "df = pd.DataFrame(list_of_feat, columns=['Best_Features'])\n",
        "\n",
        "#df.to_csv(\"Filter_Method_Sequential_feat_Selection_KNN.csv\")\n",
        "\n",
        "list_of_feat = list(df[\"Best_Features\"])\n",
        "if number_of_feat < len(list_of_feat):\n",
        "  list_of_feat = list_of_feat[:number_of_feat]\n",
        "  '''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###All features"
      ],
      "metadata": {
        "id": "cOSqGvJv7wOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_feat = list(X.columns)"
      ],
      "metadata": {
        "id": "nyQhnST17zbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAQ71nBV-p7E"
      },
      "source": [
        "###Feature list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKwGHUuo-qnn"
      },
      "outputs": [],
      "source": [
        "dfcolumns = pd.DataFrame(list_of_feat)\n",
        "print(dfcolumns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0Akgb_na7NU"
      },
      "source": [
        "#Data Spliting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uL-Rbjkl_Ath"
      },
      "outputs": [],
      "source": [
        "X_new = new_dataset[list_of_feat]\n",
        "y_new = new_dataset[target]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5l3a377_FOs"
      },
      "outputs": [],
      "source": [
        "X_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Akn6fiaQ_JON"
      },
      "outputs": [],
      "source": [
        "y_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLN_vK-s_QnK"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size = 0.2, random_state = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqYsoblRogp2"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqElxazTzXxp"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BydV84Diooxp"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEe2R0yAoifa"
      },
      "source": [
        "#ADABOOST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bz_yVJaXod8O"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_defult = AdaBoostClassifier(random_state=0)\n",
        "ada_defult.fit(X_train, y_train)\n",
        "y_pred = ada_defult.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(ada_defult,1,'AdaBoostClassifier')]=accuracy_score(y_test, y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3wRG3rxotzH"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "N=50\n",
        "k_range = range (1,N+1)\n",
        "scores={}\n",
        "scores_list = []\n",
        "for k in k_range:\n",
        "  classifier = AdaBoostClassifier(n_estimators=k,random_state=0)\n",
        "  classifier.fit(X_train, y_train)\n",
        "  y_pred=classifier.predict(X_test)\n",
        "  scores[k] = accuracy_score(y_test,y_pred)\n",
        "  scores_list.append(accuracy_score(y_test,y_pred))\n",
        "  print(str(k)+\"/\"+str(N)+\" round completed......................... Accurecy: \"+str(accuracy_score(y_test,y_pred)))\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "#plot the relationship between K and the testing accuracy\n",
        "plt.figure(figsize = (25,10))\n",
        "plt.plot(k_range,scores_list)\n",
        "plt.xlabel('Value of n_estimators')\n",
        "plt.ylabel ('Testing Accuracy')\n",
        "\n",
        "\n",
        "\n",
        "print(\"The best n_estimators:\")\n",
        "best_estimator=list(scores.keys())[scores_list.index(max(scores_list))]\n",
        "print(best_estimator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctzoGCmFo0In"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_best_estimator = AdaBoostClassifier(n_estimators=best_estimator,random_state=0)\n",
        "ada_best_estimator.fit(X_train, y_train)\n",
        "y_pred = ada_best_estimator.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(ada_best_estimator,1,'AdaBoostClassifier')]=accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQhATacCo1AD"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJybpt_UvCU0"
      },
      "source": [
        "#Graddient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgmvc8A_o18j"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "\n",
        "gradBoost_default = GradientBoostingClassifier(random_state=0)\n",
        "gradBoost_default.fit(X_train, y_train)\n",
        "y_pred = gradBoost_default.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(gradBoost_default,2,'GradientBoostingClassifier')]=accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9brp-qNgo-tX"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "N=50\n",
        "k_range = range (1,N+1)\n",
        "scores={}\n",
        "scores_list = []\n",
        "for k in k_range:\n",
        "  classifier = GradientBoostingClassifier(n_estimators=k,random_state=0)\n",
        "  classifier.fit(X_train, y_train)\n",
        "  y_pred=classifier.predict(X_test)\n",
        "  scores[k] = accuracy_score(y_test,y_pred)\n",
        "  scores_list.append(accuracy_score(y_test,y_pred))\n",
        "  print(str(k)+\"/\"+str(N)+\" round completed......................... Accurecy: \"+str(accuracy_score(y_test,y_pred)))\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (25,10))\n",
        "plt.plot(k_range,scores_list)\n",
        "plt.xlabel('Value of n_estimators')\n",
        "plt.ylabel ('Testing Accuracy')\n",
        "\n",
        "\n",
        "\n",
        "print(\"The best n_estimators:\")\n",
        "best_estimator=list(scores.keys())[scores_list.index(max(scores_list))]\n",
        "print(best_estimator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtO5QAkHpDAy"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "N=14\n",
        "k_range = range (1,N+1)\n",
        "scores={}\n",
        "scores_list = []\n",
        "for k in k_range:\n",
        "  classifier = GradientBoostingClassifier(max_depth=k,random_state=0)\n",
        "  classifier.fit(X_train, y_train)\n",
        "  y_pred=classifier.predict(X_test)\n",
        "  scores[k] = accuracy_score(y_test,y_pred)\n",
        "  scores_list.append(accuracy_score(y_test,y_pred))\n",
        "  print(str(k)+\"/\"+str(N)+\" round completed......................... Accurecy: \"+str(accuracy_score(y_test,y_pred)))\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (25,10))\n",
        "plt.plot(k_range,scores_list)\n",
        "plt.xlabel('Value of n_estimators')\n",
        "plt.ylabel ('Testing Accuracy')\n",
        "\n",
        "\n",
        "\n",
        "print(\"The best Depth:\")\n",
        "best_depth=list(scores.keys())[scores_list.index(max(scores_list))]\n",
        "print(best_depth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFg4mMYepGXx"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gradBoost_estimator = GradientBoostingClassifier(n_estimators=best_estimator,random_state=0)\n",
        "gradBoost_estimator.fit(X_train, y_train)\n",
        "y_pred = gradBoost_estimator.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(gradBoost_estimator,2,'GradientBoostingClassifier')]=accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS_ycrCQpI09"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gradBoost_depth = GradientBoostingClassifier(max_depth=best_depth,random_state=0)\n",
        "gradBoost_depth.fit(X_train, y_train)\n",
        "y_pred = gradBoost_depth.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "\n",
        "result[(gradBoost_depth,2,'GradientBoostingClassifier')]=accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GU8wNje3pJjR"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gradBoost_all = GradientBoostingClassifier(n_estimators=best_estimator,max_depth=best_depth,random_state=0)\n",
        "gradBoost_all.fit(X_train, y_train)\n",
        "y_pred = gradBoost_all.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "\n",
        "result[(gradBoost_all,2,'GradientBoostingClassifier')]=accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThoTlhc4pRJT"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgMT_U8gvM7F"
      },
      "source": [
        "#Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4GMunEvtWk2"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_default = RandomForestClassifier(random_state=0)\n",
        "rf_default.fit(X_train, y_train)\n",
        "y_pred=rf_default.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(rf_default,3,'RandomForestClassifier')]=accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ve34YgestieE"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "N=150\n",
        "k_range = range (1,N+1)\n",
        "scores={}\n",
        "scores_list = []\n",
        "for k in k_range:\n",
        "  classifier = RandomForestClassifier(n_estimators=k,random_state=0)\n",
        "  classifier.fit(X_train, y_train)\n",
        "  y_pred=classifier.predict(X_test)\n",
        "  scores[k] = accuracy_score(y_test,y_pred)\n",
        "  scores_list.append(accuracy_score(y_test,y_pred))\n",
        "  print(str(k)+\"/\"+str(N)+\" round completed......................... Accurecy: \"+str(accuracy_score(y_test,y_pred)))\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (25,10))\n",
        "plt.plot(k_range,scores_list)\n",
        "plt.xlabel('Value of n_estimators')\n",
        "plt.ylabel ('Testing Accuracy')\n",
        "\n",
        "\n",
        "\n",
        "print(\"The best n_estimators:\")\n",
        "best_estimator=list(scores.keys())[scores_list.index(max(scores_list))]\n",
        "print(best_estimator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVfKbSTWtnny"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "N=30\n",
        "k_range = range (1,N+1)\n",
        "scores={}\n",
        "scores_list = []\n",
        "for k in k_range:\n",
        "  classifier = RandomForestClassifier(max_depth=k,random_state=0)\n",
        "  classifier.fit(X_train, y_train)\n",
        "  y_pred=classifier.predict(X_test)\n",
        "  scores[k] = accuracy_score(y_test,y_pred)\n",
        "  scores_list.append(accuracy_score(y_test,y_pred))\n",
        "  print(str(k)+\"/\"+str(N)+\" round completed......................... Accurecy: \"+str(accuracy_score(y_test,y_pred)))\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (25,10))\n",
        "plt.plot(k_range,scores_list)\n",
        "plt.xlabel('Value of n_estimators')\n",
        "plt.ylabel ('Testing Accuracy')\n",
        "\n",
        "\n",
        "\n",
        "print(\"The best Depth:\")\n",
        "best_depth=list(scores.keys())[scores_list.index(max(scores_list))]\n",
        "print(best_depth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMUwwEqUt2-T"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_estimator = RandomForestClassifier(n_estimators=best_estimator,random_state=0)\n",
        "rf_estimator.fit(X_train, y_train)\n",
        "y_pred=rf_estimator.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(rf_estimator,3,'RandomForestClassifier')]=accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRybS4-nt_vO"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_depth = RandomForestClassifier(max_depth=best_depth,random_state=0)\n",
        "rf_depth.fit(X_train, y_train)\n",
        "y_pred=rf_depth.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(rf_depth,3,'RandomForestClassifier')]=accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9p0fJyBtrdZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_all = RandomForestClassifier(n_estimators=best_estimator,max_depth=best_depth,random_state=0)\n",
        "rf_all.fit(X_train, y_train)\n",
        "y_pred=rf_all.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(rf_all,3,'RandomForestClassifier')]=accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_DChNuquB92"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRG4cBnKvZU2"
      },
      "source": [
        "#XGB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sC1lAZeeuCw8"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import xgboost as xgb\n",
        "xgb_deafult = xgb.XGBClassifier(random_state=0)\n",
        "xgb_deafult.fit(X_train.values,y_train.values)\n",
        "y_pred = xgb_deafult.predict(X_test.values)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(xgb_deafult,4,'xgboost')]=accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQYsfVO0uPMA"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "N=250\n",
        "k_range = range (1,N+1)\n",
        "scores={}\n",
        "scores_list = []\n",
        "for k in k_range:\n",
        "  xgb_classifier = xgb.XGBClassifier(n_estimators=k,random_state=0)\n",
        "  xgb_classifier.fit(X_train.values, y_train.values)\n",
        "  y_pred=xgb_classifier.predict(X_test.values)\n",
        "  scores[k] = accuracy_score(y_test,y_pred)\n",
        "  scores_list.append(accuracy_score(y_test,y_pred))\n",
        "  print(str(k)+\"/\"+str(N)+\" round completed......................... Accurecy: \"+str(accuracy_score(y_test,y_pred)))\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (25,10))\n",
        "plt.plot(k_range,scores_list)\n",
        "plt.xlabel('Value of n_estimators')\n",
        "plt.ylabel ('Testing Accuracy')\n",
        "\n",
        "\n",
        "\n",
        "print(\"The best n_estimators:\")\n",
        "best_estimator=list(scores.keys())[scores_list.index(max(scores_list))]\n",
        "print(best_estimator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABex6e-3kxuX"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "N=30\n",
        "k_range = range (1,N+1)\n",
        "scores={}\n",
        "scores_list = []\n",
        "for k in k_range:\n",
        "  xgb_classifier = xgb.XGBClassifier(max_depth=k,random_state=0)\n",
        "  xgb_classifier.fit(X_train.values, y_train.values)\n",
        "  y_pred=xgb_classifier.predict(X_test.values)\n",
        "  scores[k] = accuracy_score(y_test,y_pred)\n",
        "  scores_list.append(accuracy_score(y_test,y_pred))\n",
        "  print(str(k)+\"/\"+str(N)+\" round completed......................... Accurecy: \"+str(accuracy_score(y_test,y_pred)))\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (25,10))\n",
        "plt.plot(k_range,scores_list)\n",
        "plt.xlabel('Value of n_estimators')\n",
        "plt.ylabel ('Testing Accuracy')\n",
        "\n",
        "\n",
        "\n",
        "print(\"The best depth:\")\n",
        "best_depth=list(scores.keys())[scores_list.index(max(scores_list))]\n",
        "print(best_depth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfBp-uGXuaKY"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "xgb_depth = xgb.XGBClassifier(max_depth=best_depth,random_state=0)\n",
        "xgb_depth.fit(X_train.values,y_train.values)\n",
        "y_pred = xgb_depth.predict(X_test.values)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(xgb_depth,4,'xgboost')]=accuracy_score(y_test, y_pred)\n",
        "print(xgb_depth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qBm-yTqzOsi"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "xgb_estimator = xgb.XGBClassifier(n_estimators=best_estimator,random_state=0)\n",
        "xgb_estimator.fit(X_train.values,y_train.values)\n",
        "y_pred = xgb_estimator.predict(X_test.values)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(xgb_estimator,4,'xgboost')]=accuracy_score(y_test, y_pred)\n",
        "print(xgb_estimator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW-0PfT-zX3C"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "xgb_all = xgb.XGBClassifier(n_estimators=best_estimator,max_depth=best_depth,random_state=0)\n",
        "xgb_all.fit(X_train.values,y_train.values)\n",
        "y_pred = xgb_all.predict(X_test.values)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(xgb_all,4,'xgboost')]=accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6_icFWdauSI"
      },
      "source": [
        "#KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GALUBYbaz1z"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn_default = KNeighborsClassifier()\n",
        "knn_default.fit(X_train, y_train)\n",
        "y_pred=knn_default.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(knn_default,5,'KNeighborsClassifier')]=accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpSsmYDHa-VF"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "Neighbors=105\n",
        "k_range = range (1,Neighbors+1)\n",
        "scores={}\n",
        "scores_list = []\n",
        "for k in k_range:\n",
        "  knn = KNeighborsClassifier(n_neighbors=k)\n",
        "  knn.fit(X_train, y_train)\n",
        "  y_pred=knn.predict(X_test)\n",
        "  scores[k] = accuracy_score(y_test,y_pred)\n",
        "  scores_list.append(accuracy_score(y_test,y_pred))\n",
        "  print(str(k)+\"/\"+str(Neighbors)+\" round completed......................... Accurecy: \"+str(accuracy_score(y_test,y_pred)))\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (25,10))\n",
        "plt.plot(k_range,scores_list)\n",
        "plt.xlabel('Value of n_estimators')\n",
        "plt.ylabel ('Testing Accuracy')\n",
        "\n",
        "\n",
        "\n",
        "print(\"The best Depth:\")\n",
        "best=list(scores.keys())[scores_list.index(max(scores_list))]\n",
        "print(best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIVJZYV8bCjo"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn_neighbors = KNeighborsClassifier(n_neighbors=best)\n",
        "knn_neighbors.fit(X_train, y_train)\n",
        "y_pred=knn_neighbors.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(knn_neighbors,5,'KNeighborsClassifier')]=accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Btki9jRvc1Y"
      },
      "source": [
        "#NB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRmd7ve-ubcd"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "nb_deafult = GaussianNB()\n",
        "nb_deafult.fit(X_train, y_train)\n",
        "y_pred = nb_deafult.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(nb_deafult,6,'GaussianNB')]=accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqsWPAB3uv72"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FEsxCdvkg2D"
      },
      "source": [
        "#Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rx7qCLU14Aim"
      },
      "outputs": [],
      "source": [
        "\n",
        "models=[]\n",
        "\n",
        "for i in result:\n",
        "  models.append(i[0])\n",
        "  print(i[0],i[1],\" : \",result[i])\n",
        "  print(\"---------------------------------------------------------------\")\n",
        "  print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slItYG8uLOFi"
      },
      "outputs": [],
      "source": [
        "sorted_list=[]\n",
        "sorted_list = sorted(result, key=result.get,reverse=True)\n",
        "\n",
        "for i in sorted_list:\n",
        "  print(i,\"  : \",result[i])\n",
        "  print(\"-------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(sorted_list)\n",
        "\n",
        "\n",
        "flage=[]\n",
        "best_models=[]\n",
        "it=0\n",
        "\n",
        "for i in sorted_list:\n",
        "  if it==4:\n",
        "    break\n",
        "\n",
        "  if i[1] not in flage:\n",
        "    best_models.append((i[0],i[2]))\n",
        "    flage.append(i[1])\n",
        "    it+=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SR0XBsJDMi_1"
      },
      "outputs": [],
      "source": [
        "print(\"best_models:\")\n",
        "for i in best_models:\n",
        "  print(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQCaVYKA5acD"
      },
      "outputs": [],
      "source": [
        "len(best_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PooiLPlL9WO"
      },
      "source": [
        "#Performance parameter for each class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9nWU11PMDQ8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def confusion_details(y_test,y_pred):\n",
        "    labels = list(set(y_test))\n",
        "    labels.sort()\n",
        "\n",
        "    print(\"Total labels: %s -> %s\" % (len(labels), labels))\n",
        "\n",
        "    df = pd.DataFrame(\n",
        "        data=confusion_matrix(y_test, y_pred, labels=labels),\n",
        "        columns=labels,\n",
        "        index=labels\n",
        "    )\n",
        "\n",
        "    print(df)\n",
        "\n",
        "    print()\n",
        "    print(\"----------------------------------------------------------------------------------------\")\n",
        "    print(\"----------------------------------------------------------------------------------------\")\n",
        "    print()\n",
        "    #\n",
        "    # Local (metrics per class)\n",
        "    #\n",
        "    tps = {}\n",
        "    fps = {}\n",
        "    fns = {}\n",
        "    tns = {}\n",
        "\n",
        "    precision_local = {}\n",
        "    recall_local = {}\n",
        "    f1_local = {}\n",
        "    accuracy_local = {}\n",
        "    specificity_local={}\n",
        "\n",
        "    for label in labels:\n",
        "        tps[label] = df.loc[label, label]\n",
        "        fps[label] = df[label].sum() - tps[label]\n",
        "        fns[label] = df.loc[label].sum() - tps[label]\n",
        "        tns[label]=len(y_test) - (tps[label] + fps[label] + fns[label])\n",
        "        \n",
        "        tp, fp, fn, tn = tps[label], fps[label], fns[label], tns[label]\n",
        "        \n",
        "        precision_local[label] = tp / (tp + fp) if (tp + fp) > 0. else 0.\n",
        "        specificity_local[label] = tn / (tn + fp) if (tn + fp) > 0. else 0.\n",
        "        recall_local[label] = tp / (tp + fn) if (tp + fp) > 0. else 0.\n",
        "        p, r = precision_local[label], recall_local[label]\n",
        "        \n",
        "        f1_local[label] = 2. * p * r / (p + r) if (p + r) > 0. else 0.\n",
        "        accuracy_local[label] = tp / (tp + fp + fn) if (tp + fp + fn) > 0. else 0.\n",
        "\n",
        "\n",
        "\n",
        "    print(\"#-- Local measures --#\")\n",
        "    print(\"True Positives(TP):\", tps)\n",
        "    print(\"False Positives(FP):\", fps)\n",
        "    print(\"True Negatives(TN):\", tns)\n",
        "    print(\"False Negatives(FN):\", fns)\n",
        "    print(\"----------------------------\")\n",
        "\n",
        "    print(\"Precision:\", precision_local)\n",
        "    print(\"Recall/Sensitivity:\", recall_local)\n",
        "    print(\"Specificity:\",specificity_local)\n",
        "    print(\"F1-Score:\", f1_local)\n",
        "    print(\"Accuracy:\", accuracy_local)\n",
        "\n",
        "\n",
        "    print()\n",
        "    print(\"----------------------------------------------------------------------------------------\")\n",
        "    print(\"----------------------------------------------------------------------------------------\")\n",
        "    print()\n",
        "    #\n",
        "    # Global\n",
        "    #\n",
        "    micro_averages = {}\n",
        "    macro_averages = {}\n",
        "\n",
        "    correct_predictions = sum(tps.values())\n",
        "    true_negative=sum(tns.values())\n",
        "\n",
        "    den = sum(list(tps.values()) + list(fps.values()))\n",
        "    micro_averages[\"Precision\"] = 1. * correct_predictions / den if den > 0. else 0.\n",
        "\n",
        "    den = sum(list(tps.values()) + list(fns.values()))\n",
        "    micro_averages[\"Recall\"] = 1. * correct_predictions / den if den > 0. else 0.\n",
        "\n",
        "    den = sum(list(tns.values()) + list(fps.values()))\n",
        "    micro_averages[\"Specificity\"] = 1. * true_negative / den if den > 0. else 0.\n",
        "\n",
        "\n",
        "    micro_avg_p, micro_avg_r = micro_averages[\"Precision\"], micro_averages[\"Recall\"]\n",
        "    micro_averages[\"F1-score\"] = 2. * micro_avg_p * micro_avg_r / (micro_avg_p + micro_avg_r) if (micro_avg_p + micro_avg_r) > 0. else 0.\n",
        "\n",
        "    macro_averages[\"Precision\"] = np.mean(list(precision_local.values()))\n",
        "    macro_averages[\"Recall\"] = np.mean(list(recall_local.values()))\n",
        "    macro_averages[\"Specificity\"]=np.mean(list(specificity_local.values()))\n",
        "\n",
        "\n",
        "    macro_avg_p, macro_avg_r = macro_averages[\"Precision\"], macro_averages[\"Recall\"]\n",
        "    macro_averages[\"F1-Score\"] = 2. * macro_avg_p * macro_avg_r / (macro_avg_p + macro_avg_r) if (macro_avg_p + macro_avg_r) > 0. else 0.\n",
        "\n",
        "    total_predictions = df.values.sum()\n",
        "    accuracy_global = correct_predictions / total_predictions if total_predictions > 0. else 0.\n",
        "\n",
        "    print(\"#-- Global measures --#\")\n",
        "    print(\"Micro-Averages:\", micro_averages)\n",
        "    print(\"Macro-Averages:\", macro_averages)\n",
        "    print(\"Correct predictions:\", correct_predictions)\n",
        "    print(\"Total predictions:\", total_predictions)\n",
        "    print(\"Accuracy:\", accuracy_global)\n",
        "\n",
        "\n",
        "    print()\n",
        "    print(\"----------------------------------------------------------------------------------------\")\n",
        "    print(\"----------------------------------------------------------------------------------------\")\n",
        "    print()\n",
        "\n",
        "\n",
        "\n",
        "    accuracy_local_new = {}\n",
        "    for label in labels:\n",
        "        tp, fp, fn, tn = tps[label], fps[label], fns[label], tns[label]\n",
        "        accuracy_local_new[label] = (tp + tn) / (tp + fp + fn + tn) if (tp + fp + fn + tn) > 0. else 0.\n",
        "\n",
        "    total_true = sum(list(tps.values()) + list(tns.values()))\n",
        "    total_predictions = sum(list(tps.values()) + list(tns.values()) + list(fps.values()) + list(fns.values()))\n",
        "    accuracy_global_new = 1. * total_true / total_predictions if total_predictions > 0. else 0.\n",
        "\n",
        "    print(\"Accuracy (per class), with TNs:\", accuracy_local_new)\n",
        "    print(\"Accuracy (per class), without TNs:\", accuracy_local)\n",
        "    print(\"Accuracy (global), with TNs:\", accuracy_global_new)\n",
        "    print(\"Accuracy (global), without TNs:\", accuracy_global)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mKUfZr2MaMp"
      },
      "outputs": [],
      "source": [
        "new_keys_10=models_check_box(models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnHsPRwOMIGQ"
      },
      "outputs": [],
      "source": [
        "for i in range(len(new_keys_10)):\n",
        "  if new_keys_10[i].value ==True:\n",
        "    print(\"\\n\")\n",
        "    print(\"_________________________________________________\",models[i],\"_______________________________________________\")\n",
        "\n",
        "    if str(models[i])[:3] == \"XGB\":\n",
        "      y_pred = models[i].predict(X_test.values)\n",
        "      confusion_details(y_test,y_pred)\n",
        "    \n",
        "    else:\n",
        "      y_pred = models[i].predict(X_test)\n",
        "      confusion_details(y_test,y_pred)\n",
        "    print('\\n')\n",
        "    print(\"-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "    print(\"-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "    print(\"-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "    print('\\n')\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znXNx2bgUvtd"
      },
      "source": [
        "#Testing Accuracy For Best 4 Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2j9CXuBkUuiR"
      },
      "outputs": [],
      "source": [
        "\n",
        "for i in best_models:\n",
        "  print(\"--------------------------------------------------\")\n",
        "  print()\n",
        "  if str(i[0])[:3] == \"XGB\":\n",
        "    y_pred=i[0].predict(X_test.values)\n",
        "  else:\n",
        "    y_pred=i[0].predict(X_test)\n",
        "  print(confusion_matrix(y_test, y_pred))\n",
        "  print(classification_report(y_test,y_pred))\n",
        "  print(\"Accurecy: \",accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxxWSX26jsGT"
      },
      "source": [
        "#SHAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUhJ7dWajur-"
      },
      "outputs": [],
      "source": [
        "!pip install shap\n",
        "import shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POk0kUrdB-Y4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def SHAP_EXP(model,graph_feat):\n",
        "  print(\"Models: \",model)\n",
        "\n",
        "  explainer = shap.Explainer(model.predict, X_test)\n",
        "\n",
        "  shap_values1 = explainer(X_test)\n",
        "  features_names=list_of_feat\n",
        "\n",
        "  if 'Subjects' in features_names:\n",
        "    features_names.pop(0)\n",
        "\n",
        "\n",
        "  shap.plots.bar(shap_values1,max_display=graph_feat[\"max_display\"])\n",
        "\n",
        "  print(\"---------------------\")\n",
        "\n",
        "  shap.summary_plot(shap_values1,max_display=graph_feat[\"max_display\"],feature_names=features_names)\n",
        "\n",
        "  print(\"---------------------\")\n",
        "\n",
        "  print(\"Local Explaination\")\n",
        "  shap.plots.waterfall(shap_values1[graph_feat[\"shap_values Index\"]],max_display=graph_feat[\"max_display\"])\n",
        "\n",
        "\n",
        "  print(\"---------------------\")\n",
        "\n",
        "  shap.plots.bar(shap_values1[graph_feat[\"shap_values Index\"]],max_display=graph_feat[\"max_display\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-G5szfQDiz1"
      },
      "outputs": [],
      "source": [
        "new_keys_7=models_check_box(models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeD8HT_iDodb"
      },
      "outputs": [],
      "source": [
        "graph_feat={\n",
        "    \"max_display\":20,\n",
        "    \"shap_values Index\":2\n",
        "}\n",
        "\n",
        "for i in range(len(new_keys_7)):\n",
        "  if new_keys_7[i].value ==True:\n",
        "    SHAP_EXP(models[i],graph_feat)\n",
        "    print(\"---------------------------------------------------------\")\n",
        "    print(\"---------------------------------------------------------\")\n",
        "    print(\"---------------------------------------------------------\")\n",
        "    print(\"---------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23z3A3f1ol6d"
      },
      "source": [
        "#Confusion Matrix For Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NfXI6K5aqG6"
      },
      "outputs": [],
      "source": [
        "def Conf_Mat(X_,y_,attb,keys):\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "  from yellowbrick.classifier import ConfusionMatrix\n",
        "  import matplotlib.dates as dates\n",
        "\n",
        "  fig = plt.figure(figsize=(attb[\"Fig Height\"],attb[\"Fig weidth\"]))\n",
        "  ax = fig.add_subplot(111)\n",
        "\n",
        "\n",
        "  for i in range(len(keys)):\n",
        "    if keys[i].value ==True:\n",
        "      cm = ConfusionMatrix(models[i], classes=['Reading', 'Resting', 'Walking', 'Working'],fontsize=attb[\"Confusion Matrix Inner Fontsize\"],ax=ax)\n",
        "\n",
        "      if str(models[i])[:3] == \"XGB\":\n",
        "        cm.fit(X_train.values, y_train.values)\n",
        "        cm.score(X_.values, y_.values)\n",
        "      else:\n",
        "        cm.fit(X_train, y_train)\n",
        "        cm.score(X_, y_)\n",
        "      \n",
        "      cm.ax.set_xlabel(\"Predicted Class\", fontsize=attb[\"X Axis Fontsize\"],fontweight=\"bold\")\n",
        "      cm.ax.set_ylabel(\"True Class\", fontsize=attb[\"Y Axis Fontsize\"],fontweight=\"bold\")\n",
        "      cm.ax.xaxis.set_tick_params(labelsize=attb[\"X Label Fontsize\"])\n",
        "      cm.ax.yaxis.set_tick_params(labelsize=attb[\"Y Label Fontsize\"])\n",
        "      for label in ax.get_xticklabels():\n",
        "        label.set_fontweight(550)\n",
        "      for label in ax.get_yticklabels():\n",
        "        label.set_fontweight(550)\n",
        "      \n",
        "      plt.savefig(attb[\"type\"]+\"_Confusion_mat(\"+str(models[i])+\").png\")\n",
        "      cm.show()\n",
        "      print(\"--------------------\")\n",
        "      print(\"--------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3tHiraMBr_C"
      },
      "outputs": [],
      "source": [
        "new_keys_6=models_check_box(models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vsqEYhponhI"
      },
      "outputs": [],
      "source": [
        "attributes={\n",
        "   \"Fig Height\": 10,\n",
        "   \"Fig weidth\": 10, \n",
        "    \n",
        "  \"Y Axis Fontsize\" : 20,\n",
        "  \"X Axis Fontsize\" : 20,\n",
        "\n",
        "  \"Y Label Fontsize\" : 20,\n",
        "  \"X Label Fontsize\" : 20,\n",
        "\n",
        "  \"Confusion Matrix Inner Fontsize\": 18,\n",
        "  \"type\"  : \"Testing\"\n",
        "\n",
        "}\n",
        "\n",
        "Conf_Mat(X_test,y_test,attributes,new_keys_6)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Confusion Matrix for Testing ( with Percent)"
      ],
      "metadata": {
        "id": "HdAPZkWvp08K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Conf_Mat_percent(X_,y_,attb,keys):\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "  from yellowbrick.classifier import ConfusionMatrix\n",
        "  import matplotlib.dates as dates\n",
        "\n",
        "  fig = plt.figure(figsize=(attb[\"Fig Height\"],attb[\"Fig weidth\"]))\n",
        "  ax = fig.add_subplot(111)\n",
        "\n",
        "\n",
        "  for i in range(len(keys)):\n",
        "    if keys[i].value ==True:\n",
        "      cm = ConfusionMatrix(models[i], classes=['Reading', 'Resting', 'Walking', 'Working'],fontsize=attb[\"Confusion Matrix Inner Fontsize\"],ax=ax,percent=True)\n",
        "\n",
        "      if str(models[i])[:3] == \"XGB\":\n",
        "        cm.fit(X_train.values, y_train.values)\n",
        "        cm.score(X_.values, y_.values)\n",
        "      else:\n",
        "        cm.fit(X_train, y_train)\n",
        "        cm.score(X_, y_)\n",
        "      \n",
        "      cm.ax.set_xlabel(\"Predicted Class\", fontsize=attb[\"X Axis Fontsize\"],fontweight=\"bold\")\n",
        "      cm.ax.set_ylabel(\"True Class\", fontsize=attb[\"Y Axis Fontsize\"],fontweight=\"bold\")\n",
        "      cm.ax.xaxis.set_tick_params(labelsize=attb[\"X Label Fontsize\"])\n",
        "      cm.ax.yaxis.set_tick_params(labelsize=attb[\"Y Label Fontsize\"])\n",
        "      for label in ax.get_xticklabels():\n",
        "        label.set_fontweight(550)\n",
        "      for label in ax.get_yticklabels():\n",
        "        label.set_fontweight(550)\n",
        "      \n",
        "      plt.savefig(attb[\"type\"]+\"_Confusion_mat(\"+str(models[i])+\").png\")\n",
        "      cm.show()\n",
        "      print(\"--------------------\")\n",
        "      print(\"--------------------\")"
      ],
      "metadata": {
        "id": "NjmCvk6Sp_vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_keys_20=models_check_box(models)"
      ],
      "metadata": {
        "id": "zkPx8YfsqHUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attributes={\n",
        "   \"Fig Height\": 10,\n",
        "   \"Fig weidth\": 10, \n",
        "    \n",
        "  \"Y Axis Fontsize\" : 20,\n",
        "  \"X Axis Fontsize\" : 20,\n",
        "\n",
        "  \"Y Label Fontsize\" : 20,\n",
        "  \"X Label Fontsize\" : 20,\n",
        "\n",
        "  \"Confusion Matrix Inner Fontsize\": 18,\n",
        "  \"type\"  : \"Training\"\n",
        "\n",
        "}\n",
        "\n",
        "Conf_Mat_percent(X_test,y_test,attributes,new_keys_20)"
      ],
      "metadata": {
        "id": "gGdVSMcXqIK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57cQ8dzJX0Lp"
      },
      "source": [
        "#Confusion Matrix For Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDrF5oTkBcZ_"
      },
      "outputs": [],
      "source": [
        "new_keys_5=models_check_box(models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhzRUmq9X4CZ"
      },
      "outputs": [],
      "source": [
        "attributes={\n",
        "   \"Fig Height\": 10,\n",
        "   \"Fig weidth\": 10, \n",
        "    \n",
        "  \"Y Axis Fontsize\" : 20,\n",
        "  \"X Axis Fontsize\" : 20,\n",
        "\n",
        "  \"Y Label Fontsize\" : 20,\n",
        "  \"X Label Fontsize\" : 20,\n",
        "\n",
        "  \"Confusion Matrix Inner Fontsize\": 18,\n",
        "  \"type\"  : \"Training\"\n",
        "\n",
        "}\n",
        "\n",
        "Conf_Mat(X_train,y_train,attributes,new_keys_5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Confusion Matrix for Training ( with Percent)"
      ],
      "metadata": {
        "id": "yl_6PJLpqXKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_keys_21=models_check_box(models)"
      ],
      "metadata": {
        "id": "9OHxb3AAqYXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attributes={\n",
        "   \"Fig Height\": 10,\n",
        "   \"Fig weidth\": 10, \n",
        "    \n",
        "  \"Y Axis Fontsize\" : 20,\n",
        "  \"X Axis Fontsize\" : 20,\n",
        "\n",
        "  \"Y Label Fontsize\" : 20,\n",
        "  \"X Label Fontsize\" : 20,\n",
        "\n",
        "  \"Confusion Matrix Inner Fontsize\": 18,\n",
        "  \"type\"  : \"Training\"\n",
        "\n",
        "}\n",
        "\n",
        "Conf_Mat_percent(X_train,y_train,attributes,new_keys_21)"
      ],
      "metadata": {
        "id": "hSNapvifqbdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igsy215JGzfg"
      },
      "source": [
        "#ROC FOR MULTICLASS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoPKNwlqG3ge"
      },
      "outputs": [],
      "source": [
        "!pip install plotly==5.11.0\n",
        "!pip install -U kaleido\n",
        "\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def ploty_ROC(model,X,y,fig_name,graph_attributes):\n",
        "    \n",
        "    lebel_dict={\n",
        "    0: 'Reading',\n",
        "    1: 'Resting',\n",
        "    2: 'Walking',\n",
        "    3: 'Working'\n",
        "    }\n",
        "    if str(model)[:3] == \"XGB\":\n",
        "      y_scores = model.predict_proba(X.values)\n",
        "    else:\n",
        "      y_scores = model.predict_proba(X)\n",
        "    #y_scores = model.predict_proba(X)\n",
        "\n",
        "    y_onehot = pd.get_dummies(y, columns=model.classes_)\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig.add_shape(\n",
        "        type='line', line=dict(dash='dash', width=5),        \n",
        "        x0=0, x1=1, y0=0, y1=1\n",
        "    )\n",
        "\n",
        "    for i in range(y_scores.shape[1]):\n",
        "        y_true = y_onehot.iloc[:, i]\n",
        "        y_score = y_scores[:, i]\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "        auc_score = roc_auc_score(y_true, y_score)\n",
        "        name = f\"{lebel_dict[y_onehot.columns[i]]} (AUC={auc_score:.2f})\"\n",
        "        fig.add_trace(go.Scatter(x=fpr, y=tpr, name=name, mode='lines'))\n",
        "\n",
        "    fig.update_layout(\n",
        "        xaxis_title='False Positive Rate',\n",
        "        yaxis_title='True Positive Rate',\n",
        "        yaxis=dict(scaleanchor=\"x\", scaleratio=1),\n",
        "        xaxis=dict(constrain='domain'),\n",
        "        width=1000, height=1000,\n",
        "        font=dict(\n",
        "            family=\"Arial Black, monospace\",\n",
        "            size=graph_attributes[\"Font Size\"],\n",
        "            color=graph_attributes[\"Font Color\"]\n",
        "        ),\n",
        "        legend=dict(\n",
        "            x=0.62,\n",
        "            y=0.05,\n",
        "            traceorder=\"reversed\",\n",
        "            title_font_family=\"Arial Black\",\n",
        "            font=dict(\n",
        "                family=\"Arial Black, monospace\",\n",
        "                size=graph_attributes[\"Legend Font Size\"],\n",
        "                color=graph_attributes[\"Legend Font Color\"]\n",
        "            ),\n",
        "            bgcolor=graph_attributes[\"Legend bgcolor\"],\n",
        "            bordercolor=graph_attributes[\"Legend bordercolor\"],\n",
        "            borderwidth=graph_attributes[\"Legend borderwidth\"]\n",
        "        ),\n",
        "        #plot_bgcolor=\"\",\n",
        "    )\n",
        "    fig.update_xaxes(showline=True, linewidth=2, linecolor='black', tickfont_family=\"Arial Black\")\n",
        "    fig.update_yaxes(showline=True, linewidth=2, linecolor='black',tickfont_family=\"Arial Black\")\n",
        "\n",
        "    fig.show()\n",
        "    fig.write_image(fig_name+\".png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wa81Tk8naet6"
      },
      "outputs": [],
      "source": [
        "new_keys_9=models_check_box(models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBCyRaVyZ-rG"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Change the attriutes for graph\n",
        "graph_attributes={\n",
        "    \"Font Size\"  : 25,\n",
        "    \"Font Color\" : \"black\",\n",
        "    \"Legend Font Size\"  : 25,\n",
        "    \"Legend Font Color\" : \"black\",\n",
        "    \"Legend bgcolor\"    : \"LightSteelBlue\",\n",
        "    \"Legend bordercolor\": \"White\",\n",
        "    \"Legend borderwidth\": 1\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(new_keys_9)):\n",
        "  if new_keys_9[i].value ==True:\n",
        "    print(models[i])\n",
        "    fig1=ploty_ROC(models[i],X_train,y_train,str(models[i]),graph_attributes)\n",
        "    fig2=ploty_ROC(models[i],X_test,y_test,str(models[i]),graph_attributes)\n",
        "    print(\"---------------------------------------------------------\")\n",
        "    print(\"---------------------------------------------------------\")\n",
        "    print(\"---------------------------------------------------------\")\n",
        "    print(\"---------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtgWbvJsEwMt"
      },
      "source": [
        "#Cross_val_score function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3unb_F6vE39m"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "k=5\n",
        "for i in result:\n",
        "  print(i[0],\" -> Accuracy: \",result[i])\n",
        "  l=list(cross_val_score(i[0],X_new, y_new,cv=k))\n",
        "  avg=sum(l)/k\n",
        "  print(i[0],\" -> AVG Accurecy After CV: \"+str(avg)+ \" (For \"+str(k)+\" Fold)\")\n",
        "  print(\"--------------------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDu8Ipdm8qaV"
      },
      "source": [
        "# **LIME**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aokroGxh8oj9"
      },
      "outputs": [],
      "source": [
        "!pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FaSwsYC4hL9"
      },
      "outputs": [],
      "source": [
        "\n",
        "  def LIME_EXP(model,row):\n",
        "    import lime\n",
        "    from lime import lime_tabular\n",
        "\n",
        "\n",
        "    if str(model)[:3] == \"XGB\":\n",
        "\n",
        "      \"\"\"explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "        X_train.values,\n",
        "        feature_names=list(list(X_new.columns)),                                         \n",
        "        class_names=['Reading', 'Resting', 'Walking', 'Working']\n",
        "        )\n",
        "      \n",
        "      exp = explainer.explain_instance(X_test.iloc[row, :].values,\n",
        "                                 model.predict_proba,\n",
        "                                 num_features=6,\n",
        "                                 top_labels=2)\"\"\"\n",
        "      print(\"Plz RUN XGboost cell....\")\n",
        "      return None\n",
        "\n",
        "\n",
        "\n",
        "    else:\n",
        "      explainer = lime_tabular.LimeTabularExplainer(\n",
        "        training_data=np.array(X_train),\n",
        "        feature_names=list(X_new.columns),\n",
        "        class_names=['Reading', 'Resting', 'Walking', 'Working'],\n",
        "        mode='classification'\n",
        "        )\n",
        "\n",
        "      exp = explainer.explain_instance(X_test.iloc[row],\n",
        "                                      model.predict_proba,               \n",
        "                                      num_features=6,\n",
        "                                      top_labels=4)\n",
        "    \n",
        "\n",
        "\n",
        "    exp.show_in_notebook(show_table=True, show_all=True)\n",
        "\n",
        "\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    with plt.style.context(\"ggplot\"):\n",
        "        exp.as_pyplot_figure()\n",
        "\n",
        "\n",
        "    from IPython.display import HTML\n",
        "    html_data = exp.as_html()\n",
        "    HTML(data=html_data)\n",
        "\n",
        "    exp.save_to_file(str(model)+\".html\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32KcHp_C40qQ"
      },
      "outputs": [],
      "source": [
        "row = int(input(\"Enter the index of row to explain: \"))      # the index of row to be explained in LIME\n",
        "\n",
        "\n",
        "new_keys_8=models_check_box(models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0b1o-1mBtnW"
      },
      "outputs": [],
      "source": [
        "for i in range(len(new_keys_8)):\n",
        "  if new_keys_8[i].value ==True:\n",
        "    print(models[i])\n",
        "    LIME_EXP(models[i],row)\n",
        "    print(\"---------------------------------------------------------\")\n",
        "    print(\"---------------------------------------------------------\")\n",
        "    print(\"---------------------------------------------------------\")\n",
        "    print(\"---------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uShYm2_JU-kn"
      },
      "source": [
        "### LIME for XGboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiCt6pXuYhQU"
      },
      "outputs": [],
      "source": [
        "row = 18  ## the index of row to be explained in LIME\n",
        "\n",
        "import lime\n",
        "from lime import lime_tabular\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values,\n",
        "                                                   feature_names=list(list(X_new.columns)),\n",
        "                                                   class_names=['Reading', 'Resting', 'Walking', 'Working'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV98gDirYqlI"
      },
      "source": [
        "####Default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db_NRU-cVDfk"
      },
      "outputs": [],
      "source": [
        "\n",
        "exp = explainer.explain_instance(X_test.iloc[row, :].values,\n",
        "                                 xgb_deafult.predict_proba,\n",
        "                                 num_features=6,\n",
        "                                 top_labels=4)\n",
        "\n",
        "exp.show_in_notebook(show_table=True, show_all=False)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "with plt.style.context(\"ggplot\"):\n",
        "    exp.as_pyplot_figure()\n",
        "\n",
        "\n",
        "from IPython.display import HTML\n",
        "html_data = exp.as_html()\n",
        "HTML(data=html_data)\n",
        "\n",
        "exp.save_to_file(str(xgb_deafult)+\".html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCtRZ2soYvmm"
      },
      "source": [
        "####Best Max Depth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmErRPogYdxA"
      },
      "outputs": [],
      "source": [
        "\n",
        "exp = explainer.explain_instance(X_test.iloc[row, :].values,\n",
        "                                 xgb_depth.predict_proba,\n",
        "                                 num_features=6,\n",
        "                                 top_labels=4)\n",
        "\n",
        "exp.show_in_notebook(show_table=True, show_all=False)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "with plt.style.context(\"ggplot\"):\n",
        "    exp.as_pyplot_figure()\n",
        "\n",
        "\n",
        "from IPython.display import HTML\n",
        "html_data = exp.as_html()\n",
        "HTML(data=html_data)\n",
        "\n",
        "exp.save_to_file(str(xgb_depth)+\".html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db6bDLDKZBu6"
      },
      "source": [
        "####Best N Estimator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqJ6uclBYeb2"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "exp = explainer.explain_instance(X_test.iloc[row, :].values,\n",
        "                                 xgb_estimator.predict_proba,\n",
        "                                 num_features=6,\n",
        "                                 top_labels=4)\n",
        "\n",
        "exp.show_in_notebook(show_table=True, show_all=False)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "with plt.style.context(\"ggplot\"):\n",
        "    exp.as_pyplot_figure()\n",
        "\n",
        "\n",
        "from IPython.display import HTML\n",
        "html_data = exp.as_html()\n",
        "HTML(data=html_data)\n",
        "\n",
        "exp.save_to_file(str(xgb_estimator)+\".html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGZFn8-bZJoK"
      },
      "source": [
        "####Best Depth and Best Estimator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPeJmpyvYfAW"
      },
      "outputs": [],
      "source": [
        "\n",
        "exp = explainer.explain_instance(X_test.iloc[row, :].values,\n",
        "                                 xgb_all.predict_proba,\n",
        "                                 num_features=6,\n",
        "                                 top_labels=4)\n",
        "\n",
        "exp.show_in_notebook(show_table=True, show_all=False)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "with plt.style.context(\"ggplot\"):\n",
        "    exp.as_pyplot_figure()\n",
        "\n",
        "\n",
        "from IPython.display import HTML\n",
        "html_data = exp.as_html()\n",
        "HTML(data=html_data)\n",
        "\n",
        "exp.save_to_file(str(xgb_all)+\".html\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "rrR11h2K-aJn",
        "85wbxGFW-kGZ",
        "YAQ71nBV-p7E",
        "jEe2R0yAoifa",
        "FJybpt_UvCU0",
        "AgMT_U8gvM7F",
        "jRG4cBnKvZU2",
        "K6_icFWdauSI",
        "_Btki9jRvc1Y",
        "znXNx2bgUvtd",
        "zxxWSX26jsGT",
        "XtgWbvJsEwMt"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}