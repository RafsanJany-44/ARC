{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:51:38.465626Z","iopub.execute_input":"2024-01-01T03:51:38.466691Z","iopub.status.idle":"2024-01-01T03:51:38.470554Z","shell.execute_reply.started":"2024-01-01T03:51:38.466659Z","shell.execute_reply":"2024-01-01T03:51:38.469643Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:51:39.020605Z","iopub.execute_input":"2024-01-01T03:51:39.021485Z","iopub.status.idle":"2024-01-01T03:51:39.025674Z","shell.execute_reply.started":"2024-01-01T03:51:39.021451Z","shell.execute_reply":"2024-01-01T03:51:39.024575Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#pip install pandas openpyxl\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T03:51:40.396356Z","iopub.execute_input":"2024-01-01T03:51:40.397294Z","iopub.status.idle":"2024-01-01T03:51:40.401134Z","shell.execute_reply.started":"2024-01-01T03:51:40.397257Z","shell.execute_reply":"2024-01-01T03:51:40.400081Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_excel('/kaggle/input/ecg-ds-health-care/DS-Healthcare.xlsx')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T04:01:53.737297Z","iopub.execute_input":"2024-01-01T04:01:53.737676Z","iopub.status.idle":"2024-01-01T04:06:23.796835Z","shell.execute_reply.started":"2024-01-01T04:01:53.737633Z","shell.execute_reply":"2024-01-01T04:06:23.796019Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"data.head(20)","metadata":{},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Unnamed: 1</th>\n","      <th>Unnamed: 2</th>\n","      <th>Unnamed: 3</th>\n","      <th>Unnamed: 4</th>\n","      <th>Unnamed: 5</th>\n","      <th>Unnamed: 6</th>\n","      <th>Unnamed: 7</th>\n","      <th>Unnamed: 8</th>\n","      <th>Unnamed: 9</th>\n","      <th>Unnamed: 10</th>\n","      <th>Unnamed: 11</th>\n","      <th>Unnamed: 12</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Subject</td>\n","      <td>Type</td>\n","      <td>Cycle</td>\n","      <td>Time</td>\n","      <td>RR-I</td>\n","      <td>HR</td>\n","      <td>R-H</td>\n","      <td>P-H</td>\n","      <td>QRS</td>\n","      <td>PRQ</td>\n","      <td>QT</td>\n","      <td>QTC</td>\n","      <td>ST</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>E1</td>\n","      <td>ischemic</td>\n","      <td>1</td>\n","      <td>1.072</td>\n","      <td>0.932</td>\n","      <td>64.377682</td>\n","      <td>3.305</td>\n","      <td>3.81</td>\n","      <td>0.088</td>\n","      <td>0.16</td>\n","      <td>0.384</td>\n","      <td>0.397762</td>\n","      <td>0.296</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>E1</td>\n","      <td>ischemic</td>\n","      <td>2</td>\n","      <td>2.004</td>\n","      <td>1.004</td>\n","      <td>59.760956</td>\n","      <td>3.285</td>\n","      <td>3.81</td>\n","      <td>0.1</td>\n","      <td>0.16</td>\n","      <td>0.64</td>\n","      <td>0.638724</td>\n","      <td>0.54</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>E1</td>\n","      <td>ischemic</td>\n","      <td>3</td>\n","      <td>3.008</td>\n","      <td>0.996</td>\n","      <td>60.240964</td>\n","      <td>3.34</td>\n","      <td>3.83</td>\n","      <td>0.128</td>\n","      <td>0.148</td>\n","      <td>0.628</td>\n","      <td>0.62926</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>E1</td>\n","      <td>ischemic</td>\n","      <td>4</td>\n","      <td>4.004</td>\n","      <td>1.016</td>\n","      <td>59.055118</td>\n","      <td>3.325</td>\n","      <td>3.81</td>\n","      <td>0.108</td>\n","      <td>0.172</td>\n","      <td>0.388</td>\n","      <td>0.384933</td>\n","      <td>0.28</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>E1</td>\n","      <td>ischemic</td>\n","      <td>5</td>\n","      <td>5.02</td>\n","      <td>0.972</td>\n","      <td>61.728395</td>\n","      <td>3.29</td>\n","      <td>3.765</td>\n","      <td>0.1</td>\n","      <td>0.16</td>\n","      <td>0.372</td>\n","      <td>0.37732</td>\n","      <td>0.272</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>E1</td>\n","      <td>ischemic</td>\n","      <td>6</td>\n","      <td>5.992</td>\n","      <td>1.04</td>\n","      <td>57.692308</td>\n","      <td>3.27</td>\n","      <td>3.855</td>\n","      <td>0.112</td>\n","      <td>0.168</td>\n","      <td>0.376</td>\n","      <td>0.368698</td>\n","      <td>0.264</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>E1</td>\n","      <td>ischemic</td>\n","      <td>7</td>\n","      <td>7.032</td>\n","      <td>1.028</td>\n","      <td>58.365759</td>\n","      <td>3.355</td>\n","      <td>3.93</td>\n","      <td>0.092</td>\n","      <td>0.168</td>\n","      <td>0.384</td>\n","      <td>0.378734</td>\n","      <td>0.292</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>E1</td>\n","      <td>ischemic</td>\n","      <td>8</td>\n","      <td>8.06</td>\n","      <td>0.988</td>\n","      <td>60.728745</td>\n","      <td>3.38</td>\n","      <td>3.91</td>\n","      <td>0.092</td>\n","      <td>0.164</td>\n","      <td>0.368</td>\n","      <td>0.370228</td>\n","      <td>0.276</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>E1</td>\n","      <td>ischemic</td>\n","      <td>9</td>\n","      <td>9.048</td>\n","      <td>0.96</td>\n","      <td>62.5</td>\n","      <td>3.39</td>\n","      <td>3.875</td>\n","      <td>0.104</td>\n","      <td>0.164</td>\n","      <td>0.38</td>\n","      <td>0.387836</td>\n","      <td>0.276</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>E1</td>\n","      <td>ischemic</td>\n","      <td>10</td>\n","      <td>10.008</td>\n","      <td>0.96</td>\n","      <td>62.5</td>\n","      <td>3.365</td>\n","      <td>3.885</td>\n","      <td>0.1</td>\n","      <td>0.164</td>\n","      <td>0.38</td>\n","      <td>0.387836</td>\n","      <td>0.28</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>E1</td>\n","      <td>ischemic</td>\n","      <td>11</td>\n","      <td>10.968</td>\n","      <td>0.964</td>\n","      <td>62.240664</td>\n","      <td>3.345</td>\n","      <td>3.85</td>\n","      <td>0.088</td>\n","      <td>0.164</td>\n","      <td>0.36</td>\n","      <td>0.36666</td>\n","      <td>0.272</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>E1</td>\n","      <td>ischemic</td>\n","      <td>12</td>\n","      <td>11.932</td>\n","      <td>0.956</td>\n","      <td>62.761506</td>\n","      <td>3.325</td>\n","      <td>3.795</td>\n","      <td>0.104</td>\n","      <td>0.16</td>\n","      <td>0.576</td>\n","      <td>0.589106</td>\n","      <td>0.472</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>E1</td>\n","      <td>ischemic</td>\n","      <td>13</td>\n","      <td>12.888</td>\n","      <td>0.984</td>\n","      <td>60.97561</td>\n","      <td>3.34</td>\n","      <td>3.875</td>\n","      <td>0.104</td>\n","      <td>0.16</td>\n","      <td>0.404</td>\n","      <td>0.407271</td>\n","      <td>0.3</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>E1</td>\n","      <td>ischemic</td>\n","      <td>14</td>\n","      <td>13.872</td>\n","      <td>1.004</td>\n","      <td>59.760956</td>\n","      <td>3.34</td>\n","      <td>3.9</td>\n","      <td>0.088</td>\n","      <td>0.168</td>\n","      <td>0.376</td>\n","      <td>0.37525</td>\n","      <td>0.288</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>E1</td>\n","      <td>ischemic</td>\n","      <td>15</td>\n","      <td>14.876</td>\n","      <td>0.94</td>\n","      <td>63.829787</td>\n","      <td>3.375</td>\n","      <td>3.925</td>\n","      <td>0.1</td>\n","      <td>0.144</td>\n","      <td>0.512</td>\n","      <td>0.528088</td>\n","      <td>0.412</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>E1</td>\n","      <td>ischemic</td>\n","      <td>16</td>\n","      <td>15.816</td>\n","      <td>0.956</td>\n","      <td>62.761506</td>\n","      <td>3.415</td>\n","      <td>3.9</td>\n","      <td>0.256</td>\n","      <td>0.164</td>\n","      <td>0.564</td>\n","      <td>0.576833</td>\n","      <td>0.488</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>E1</td>\n","      <td>ischemic</td>\n","      <td>17</td>\n","      <td>16.772</td>\n","      <td>0.976</td>\n","      <td>61.47541</td>\n","      <td>3.43</td>\n","      <td>3.85</td>\n","      <td>0.088</td>\n","      <td>0.18</td>\n","      <td>0.368</td>\n","      <td>0.372497</td>\n","      <td>0.28</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>E1</td>\n","      <td>ischemic</td>\n","      <td>18</td>\n","      <td>17.748</td>\n","      <td>0.96</td>\n","      <td>62.5</td>\n","      <td>3.34</td>\n","      <td>3.835</td>\n","      <td>0.088</td>\n","      <td>0.168</td>\n","      <td>0.364</td>\n","      <td>0.371506</td>\n","      <td>0.276</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>E1</td>\n","      <td>ischemic</td>\n","      <td>19</td>\n","      <td>18.708</td>\n","      <td>0.992</td>\n","      <td>60.483871</td>\n","      <td>3.355</td>\n","      <td>3.795</td>\n","      <td>0.128</td>\n","      <td>0.164</td>\n","      <td>0.396</td>\n","      <td>0.397594</td>\n","      <td>0.268</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0 Unnamed: 1 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5  \\\n","0     Subject       Type      Cycle       Time       RR-I         HR   \n","1          E1   ischemic          1      1.072      0.932  64.377682   \n","2          E1   ischemic          2      2.004      1.004  59.760956   \n","3          E1   ischemic          3      3.008      0.996  60.240964   \n","4          E1   ischemic          4      4.004      1.016  59.055118   \n","5          E1   ischemic          5       5.02      0.972  61.728395   \n","6          E1   ischemic          6      5.992       1.04  57.692308   \n","7          E1   ischemic          7      7.032      1.028  58.365759   \n","8          E1   ischemic          8       8.06      0.988  60.728745   \n","9          E1   ischemic          9      9.048       0.96       62.5   \n","10         E1   ischemic         10     10.008       0.96       62.5   \n","11         E1   ischemic         11     10.968      0.964  62.240664   \n","12         E1   ischemic         12     11.932      0.956  62.761506   \n","13         E1   ischemic         13     12.888      0.984   60.97561   \n","14         E1   ischemic         14     13.872      1.004  59.760956   \n","15         E1   ischemic         15     14.876       0.94  63.829787   \n","16         E1   ischemic         16     15.816      0.956  62.761506   \n","17         E1   ischemic         17     16.772      0.976   61.47541   \n","18         E1   ischemic         18     17.748       0.96       62.5   \n","19         E1   ischemic         19     18.708      0.992  60.483871   \n","\n","   Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11  \\\n","0         R-H        P-H        QRS        PRQ          QT         QTC   \n","1       3.305       3.81      0.088       0.16       0.384    0.397762   \n","2       3.285       3.81        0.1       0.16        0.64    0.638724   \n","3        3.34       3.83      0.128      0.148       0.628     0.62926   \n","4       3.325       3.81      0.108      0.172       0.388    0.384933   \n","5        3.29      3.765        0.1       0.16       0.372     0.37732   \n","6        3.27      3.855      0.112      0.168       0.376    0.368698   \n","7       3.355       3.93      0.092      0.168       0.384    0.378734   \n","8        3.38       3.91      0.092      0.164       0.368    0.370228   \n","9        3.39      3.875      0.104      0.164        0.38    0.387836   \n","10      3.365      3.885        0.1      0.164        0.38    0.387836   \n","11      3.345       3.85      0.088      0.164        0.36     0.36666   \n","12      3.325      3.795      0.104       0.16       0.576    0.589106   \n","13       3.34      3.875      0.104       0.16       0.404    0.407271   \n","14       3.34        3.9      0.088      0.168       0.376     0.37525   \n","15      3.375      3.925        0.1      0.144       0.512    0.528088   \n","16      3.415        3.9      0.256      0.164       0.564    0.576833   \n","17       3.43       3.85      0.088       0.18       0.368    0.372497   \n","18       3.34      3.835      0.088      0.168       0.364    0.371506   \n","19      3.355      3.795      0.128      0.164       0.396    0.397594   \n","\n","   Unnamed: 12  \n","0           ST  \n","1        0.296  \n","2         0.54  \n","3          0.5  \n","4         0.28  \n","5        0.272  \n","6        0.264  \n","7        0.292  \n","8        0.276  \n","9        0.276  \n","10        0.28  \n","11       0.272  \n","12       0.472  \n","13         0.3  \n","14       0.288  \n","15       0.412  \n","16       0.488  \n","17        0.28  \n","18       0.276  \n","19       0.268  "]},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":6,"outputs":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: xlrd in e:\\conda\\lib\\site-packages (2.0.1)\n\nNote: you may need to restart the kernel to use updated packages.\n"}]},{"cell_type":"code","source":"\nnew_column_names = data.iloc[0]  \ndata.columns = new_column_names  \ndata = data[1:]  \n","metadata":{},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"data.tail(100)","metadata":{},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Subject</th>\n","      <th>Type</th>\n","      <th>Cycle</th>\n","      <th>Time</th>\n","      <th>RR-I</th>\n","      <th>HR</th>\n","      <th>R-H</th>\n","      <th>P-H</th>\n","      <th>QRS</th>\n","      <th>PRQ</th>\n","      <th>QT</th>\n","      <th>QTC</th>\n","      <th>ST</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1048475</th>\n","      <td>M28</td>\n","      <td>arrhythmic</td>\n","      <td>644</td>\n","      <td>420.280556</td>\n","      <td>0.647222</td>\n","      <td>92.703863</td>\n","      <td>0.7</td>\n","      <td>-0.18</td>\n","      <td>0.058333</td>\n","      <td>0.147222</td>\n","      <td>0.388889</td>\n","      <td>0.483391</td>\n","      <td>0.330556</td>\n","    </tr>\n","    <tr>\n","      <th>1048476</th>\n","      <td>M28</td>\n","      <td>arrhythmic</td>\n","      <td>645</td>\n","      <td>420.927778</td>\n","      <td>0.638889</td>\n","      <td>93.913043</td>\n","      <td>0.73</td>\n","      <td>-0.195</td>\n","      <td>0.091667</td>\n","      <td>0.144444</td>\n","      <td>0.433333</td>\n","      <td>0.542137</td>\n","      <td>0.341667</td>\n","    </tr>\n","    <tr>\n","      <th>1048477</th>\n","      <td>M28</td>\n","      <td>arrhythmic</td>\n","      <td>646</td>\n","      <td>421.566667</td>\n","      <td>0.652778</td>\n","      <td>91.914894</td>\n","      <td>0.74</td>\n","      <td>-0.165</td>\n","      <td>0.058333</td>\n","      <td>0.163889</td>\n","      <td>0.394444</td>\n","      <td>0.488206</td>\n","      <td>0.336111</td>\n","    </tr>\n","    <tr>\n","      <th>1048478</th>\n","      <td>M28</td>\n","      <td>arrhythmic</td>\n","      <td>647</td>\n","      <td>422.219444</td>\n","      <td>0.655556</td>\n","      <td>91.525424</td>\n","      <td>0.6</td>\n","      <td>-0.185</td>\n","      <td>0.055556</td>\n","      <td>0.155556</td>\n","      <td>0.405556</td>\n","      <td>0.500894</td>\n","      <td>0.35</td>\n","    </tr>\n","    <tr>\n","      <th>1048479</th>\n","      <td>M28</td>\n","      <td>arrhythmic</td>\n","      <td>648</td>\n","      <td>422.875</td>\n","      <td>0.647222</td>\n","      <td>92.703863</td>\n","      <td>0.705</td>\n","      <td>-0.175</td>\n","      <td>0.061111</td>\n","      <td>0.158333</td>\n","      <td>0.416667</td>\n","      <td>0.517919</td>\n","      <td>0.355556</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1048570</th>\n","      <td>M28</td>\n","      <td>arrhythmic</td>\n","      <td>739</td>\n","      <td>482.161111</td>\n","      <td>0.652778</td>\n","      <td>91.914894</td>\n","      <td>0.66</td>\n","      <td>-0.335</td>\n","      <td>0.066667</td>\n","      <td>0.163889</td>\n","      <td>0.411111</td>\n","      <td>0.508834</td>\n","      <td>0.355556</td>\n","    </tr>\n","    <tr>\n","      <th>1048571</th>\n","      <td>M28</td>\n","      <td>arrhythmic</td>\n","      <td>740</td>\n","      <td>482.813889</td>\n","      <td>0.675</td>\n","      <td>88.888889</td>\n","      <td>0.605</td>\n","      <td>-0.345</td>\n","      <td>0.066667</td>\n","      <td>0.172222</td>\n","      <td>0.413889</td>\n","      <td>0.50377</td>\n","      <td>0.358333</td>\n","    </tr>\n","    <tr>\n","      <th>1048572</th>\n","      <td>M28</td>\n","      <td>arrhythmic</td>\n","      <td>741</td>\n","      <td>483.488889</td>\n","      <td>0.647222</td>\n","      <td>92.703863</td>\n","      <td>0.795</td>\n","      <td>-0.335</td>\n","      <td>0.055556</td>\n","      <td>0.161111</td>\n","      <td>0.397222</td>\n","      <td>0.49375</td>\n","      <td>0.341667</td>\n","    </tr>\n","    <tr>\n","      <th>1048573</th>\n","      <td>M28</td>\n","      <td>arrhythmic</td>\n","      <td>742</td>\n","      <td>484.136111</td>\n","      <td>0.647222</td>\n","      <td>92.703863</td>\n","      <td>0.825</td>\n","      <td>-0.33</td>\n","      <td>0.069444</td>\n","      <td>0.152778</td>\n","      <td>0.4</td>\n","      <td>0.497202</td>\n","      <td>0.344444</td>\n","    </tr>\n","    <tr>\n","      <th>1048574</th>\n","      <td>M28</td>\n","      <td>arrhythmic</td>\n","      <td>743</td>\n","      <td>484.783333</td>\n","      <td>0.641667</td>\n","      <td>93.506494</td>\n","      <td>0.625</td>\n","      <td>-0.3</td>\n","      <td>0.066667</td>\n","      <td>0.161111</td>\n","      <td>0.391667</td>\n","      <td>0.488947</td>\n","      <td>0.336111</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows Ã— 13 columns</p>\n","</div>"],"text/plain":["0       Subject        Type Cycle        Time      RR-I         HR    R-H  \\\n","1048475     M28  arrhythmic   644  420.280556  0.647222  92.703863    0.7   \n","1048476     M28  arrhythmic   645  420.927778  0.638889  93.913043   0.73   \n","1048477     M28  arrhythmic   646  421.566667  0.652778  91.914894   0.74   \n","1048478     M28  arrhythmic   647  422.219444  0.655556  91.525424    0.6   \n","1048479     M28  arrhythmic   648     422.875  0.647222  92.703863  0.705   \n","...         ...         ...   ...         ...       ...        ...    ...   \n","1048570     M28  arrhythmic   739  482.161111  0.652778  91.914894   0.66   \n","1048571     M28  arrhythmic   740  482.813889     0.675  88.888889  0.605   \n","1048572     M28  arrhythmic   741  483.488889  0.647222  92.703863  0.795   \n","1048573     M28  arrhythmic   742  484.136111  0.647222  92.703863  0.825   \n","1048574     M28  arrhythmic   743  484.783333  0.641667  93.506494  0.625   \n","\n","0          P-H       QRS       PRQ        QT       QTC        ST  \n","1048475  -0.18  0.058333  0.147222  0.388889  0.483391  0.330556  \n","1048476 -0.195  0.091667  0.144444  0.433333  0.542137  0.341667  \n","1048477 -0.165  0.058333  0.163889  0.394444  0.488206  0.336111  \n","1048478 -0.185  0.055556  0.155556  0.405556  0.500894      0.35  \n","1048479 -0.175  0.061111  0.158333  0.416667  0.517919  0.355556  \n","...        ...       ...       ...       ...       ...       ...  \n","1048570 -0.335  0.066667  0.163889  0.411111  0.508834  0.355556  \n","1048571 -0.345  0.066667  0.172222  0.413889   0.50377  0.358333  \n","1048572 -0.335  0.055556  0.161111  0.397222   0.49375  0.341667  \n","1048573  -0.33  0.069444  0.152778       0.4  0.497202  0.344444  \n","1048574   -0.3  0.066667  0.161111  0.391667  0.488947  0.336111  \n","\n","[100 rows x 13 columns]"]},"metadata":{}}]},{"cell_type":"code","source":"data.Type.value_counts()","metadata":{},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":["ischemic      705983\n","healthy       284849\n","arrhythmic     57742\n","Name: Type, dtype: int64"]},"metadata":{}}]},{"cell_type":"code","source":"data['Type'] = data['Type'].map({'arrhythmic': 'unhealthy', 'ischemic': 'unhealthy', 'healthy': 'healthy'})\n\n\n","metadata":{},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Subject</th>\n","      <th>Type</th>\n","      <th>Cycle</th>\n","      <th>Time</th>\n","      <th>RR-I</th>\n","      <th>HR</th>\n","      <th>R-H</th>\n","      <th>P-H</th>\n","      <th>QRS</th>\n","      <th>PRQ</th>\n","      <th>QT</th>\n","      <th>QTC</th>\n","      <th>ST</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>E1</td>\n","      <td>unhealthy</td>\n","      <td>1</td>\n","      <td>1.072</td>\n","      <td>0.932</td>\n","      <td>64.377682</td>\n","      <td>3.305</td>\n","      <td>3.81</td>\n","      <td>0.088</td>\n","      <td>0.16</td>\n","      <td>0.384</td>\n","      <td>0.397762</td>\n","      <td>0.296</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>E1</td>\n","      <td>unhealthy</td>\n","      <td>2</td>\n","      <td>2.004</td>\n","      <td>1.004</td>\n","      <td>59.760956</td>\n","      <td>3.285</td>\n","      <td>3.81</td>\n","      <td>0.1</td>\n","      <td>0.16</td>\n","      <td>0.64</td>\n","      <td>0.638724</td>\n","      <td>0.54</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>E1</td>\n","      <td>unhealthy</td>\n","      <td>3</td>\n","      <td>3.008</td>\n","      <td>0.996</td>\n","      <td>60.240964</td>\n","      <td>3.34</td>\n","      <td>3.83</td>\n","      <td>0.128</td>\n","      <td>0.148</td>\n","      <td>0.628</td>\n","      <td>0.62926</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>E1</td>\n","      <td>unhealthy</td>\n","      <td>4</td>\n","      <td>4.004</td>\n","      <td>1.016</td>\n","      <td>59.055118</td>\n","      <td>3.325</td>\n","      <td>3.81</td>\n","      <td>0.108</td>\n","      <td>0.172</td>\n","      <td>0.388</td>\n","      <td>0.384933</td>\n","      <td>0.28</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>E1</td>\n","      <td>unhealthy</td>\n","      <td>5</td>\n","      <td>5.02</td>\n","      <td>0.972</td>\n","      <td>61.728395</td>\n","      <td>3.29</td>\n","      <td>3.765</td>\n","      <td>0.1</td>\n","      <td>0.16</td>\n","      <td>0.372</td>\n","      <td>0.37732</td>\n","      <td>0.272</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["0 Subject       Type Cycle   Time   RR-I         HR    R-H    P-H    QRS  \\\n","1      E1  unhealthy     1  1.072  0.932  64.377682  3.305   3.81  0.088   \n","2      E1  unhealthy     2  2.004  1.004  59.760956  3.285   3.81    0.1   \n","3      E1  unhealthy     3  3.008  0.996  60.240964   3.34   3.83  0.128   \n","4      E1  unhealthy     4  4.004  1.016  59.055118  3.325   3.81  0.108   \n","5      E1  unhealthy     5   5.02  0.972  61.728395   3.29  3.765    0.1   \n","\n","0    PRQ     QT       QTC     ST  \n","1   0.16  0.384  0.397762  0.296  \n","2   0.16   0.64  0.638724   0.54  \n","3  0.148  0.628   0.62926    0.5  \n","4  0.172  0.388  0.384933   0.28  \n","5   0.16  0.372   0.37732  0.272  "]},"metadata":{}}]},{"cell_type":"code","source":"# Assuming your DataFrame is named 'data'\nnull_count = data.isnull().sum()\n\n# Display the count of null values for each column\nprint(null_count)\n","metadata":{},"execution_count":12,"outputs":[{"name":"stdout","output_type":"stream","text":"0\n\nSubject    0\n\nType       0\n\nCycle      0\n\nTime       0\n\nRR-I       0\n\nHR         0\n\nR-H        0\n\nP-H        0\n\nQRS        0\n\nPRQ        0\n\nQT         0\n\nQTC        0\n\nST         0\n\ndtype: int64\n"}]},{"cell_type":"code","source":"total_null_count = null_count.sum()\nprint(f'Total number of null values: {total_null_count}')\n","metadata":{},"execution_count":13,"outputs":[{"name":"stdout","output_type":"stream","text":"Total number of null values: 0\n"}]},{"cell_type":"code","source":"pip install -U imbalanced-learn\n","metadata":{},"execution_count":14,"outputs":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: imbalanced-learn in e:\\conda\\lib\\site-packages (0.11.0)\n\nRequirement already satisfied: numpy>=1.17.3 in e:\\conda\\lib\\site-packages (from imbalanced-learn) (1.24.3)\n\nRequirement already satisfied: scipy>=1.5.0 in e:\\conda\\lib\\site-packages (from imbalanced-learn) (1.10.1)\n\nRequirement already satisfied: scikit-learn>=1.0.2 in e:\\conda\\lib\\site-packages (from imbalanced-learn) (1.3.0)\n\nRequirement already satisfied: joblib>=1.1.1 in e:\\conda\\lib\\site-packages (from imbalanced-learn) (1.2.0)\n\nRequirement already satisfied: threadpoolctl>=2.0.0 in e:\\conda\\lib\\site-packages (from imbalanced-learn) (2.2.0)\n\nNote: you may need to restart the kernel to use updated packages.\n"}]},{"cell_type":"code","source":"data= data.drop('Subject', axis=1)","metadata":{},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Type</th>\n","      <th>Cycle</th>\n","      <th>Time</th>\n","      <th>RR-I</th>\n","      <th>HR</th>\n","      <th>R-H</th>\n","      <th>P-H</th>\n","      <th>QRS</th>\n","      <th>PRQ</th>\n","      <th>QT</th>\n","      <th>QTC</th>\n","      <th>ST</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>unhealthy</td>\n","      <td>1</td>\n","      <td>1.072</td>\n","      <td>0.932</td>\n","      <td>64.377682</td>\n","      <td>3.305</td>\n","      <td>3.81</td>\n","      <td>0.088</td>\n","      <td>0.16</td>\n","      <td>0.384</td>\n","      <td>0.397762</td>\n","      <td>0.296</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>unhealthy</td>\n","      <td>2</td>\n","      <td>2.004</td>\n","      <td>1.004</td>\n","      <td>59.760956</td>\n","      <td>3.285</td>\n","      <td>3.81</td>\n","      <td>0.1</td>\n","      <td>0.16</td>\n","      <td>0.64</td>\n","      <td>0.638724</td>\n","      <td>0.54</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>unhealthy</td>\n","      <td>3</td>\n","      <td>3.008</td>\n","      <td>0.996</td>\n","      <td>60.240964</td>\n","      <td>3.34</td>\n","      <td>3.83</td>\n","      <td>0.128</td>\n","      <td>0.148</td>\n","      <td>0.628</td>\n","      <td>0.62926</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>unhealthy</td>\n","      <td>4</td>\n","      <td>4.004</td>\n","      <td>1.016</td>\n","      <td>59.055118</td>\n","      <td>3.325</td>\n","      <td>3.81</td>\n","      <td>0.108</td>\n","      <td>0.172</td>\n","      <td>0.388</td>\n","      <td>0.384933</td>\n","      <td>0.28</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>unhealthy</td>\n","      <td>5</td>\n","      <td>5.02</td>\n","      <td>0.972</td>\n","      <td>61.728395</td>\n","      <td>3.29</td>\n","      <td>3.765</td>\n","      <td>0.1</td>\n","      <td>0.16</td>\n","      <td>0.372</td>\n","      <td>0.37732</td>\n","      <td>0.272</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["0       Type Cycle   Time   RR-I         HR    R-H    P-H    QRS    PRQ  \\\n","1  unhealthy     1  1.072  0.932  64.377682  3.305   3.81  0.088   0.16   \n","2  unhealthy     2  2.004  1.004  59.760956  3.285   3.81    0.1   0.16   \n","3  unhealthy     3  3.008  0.996  60.240964   3.34   3.83  0.128  0.148   \n","4  unhealthy     4  4.004  1.016  59.055118  3.325   3.81  0.108  0.172   \n","5  unhealthy     5   5.02  0.972  61.728395   3.29  3.765    0.1   0.16   \n","\n","0     QT       QTC     ST  \n","1  0.384  0.397762  0.296  \n","2   0.64  0.638724   0.54  \n","3  0.628   0.62926    0.5  \n","4  0.388  0.384933   0.28  \n","5  0.372   0.37732  0.272  "]},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Assuming your DataFrame is named 'data'\nlabel_encoder = LabelEncoder()\n\n# Apply label encoding to the 'Type' column\ndata['Type'] = label_encoder.fit_transform(data['Type'])\n","metadata":{},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"data.Type.value_counts()","metadata":{},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":["1    763725\n","0    284849\n","Name: Type, dtype: int64"]},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report\nimport numpy as np\n\n# Assuming your DataFrame is named 'data'\n# Assuming 'Type' is the column with the class labels\n\n# Drop non-numeric columns or encode them if needed\ndata_numeric = data.select_dtypes(include=['number'])\n\n# Create feature matrix X and target variable y\nX = data_numeric.drop('Type', axis=1).values\ny = data_numeric['Type'].values\n\n# Convert labels to numerical values using LabelEncoder\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Convert numerical labels to PyTorch tensors\nX_tensor = torch.tensor(X, dtype=torch.float32)\ny_tensor = torch.tensor(y_encoded, dtype=torch.long)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n\n# Print input size\nprint(\"Input size:\", X_train.size())\n\n# Define the MLP model in PyTorch\nclass MLPModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(MLPModel, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n# Instantiate the model\ninput_size = X_train.size(1)  # Adjusted input size\nhidden_size = 64  # You can adjust the hidden size as needed\nnum_classes = len(label_encoder.classes_)\nmodel = MLPModel(input_size, hidden_size, num_classes)\n\n# Print model architecture\nprint(model)\n\n# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training the model\nnum_epochs = 500\nfor epoch in range(num_epochs):\n    model.train()\n    outputs = model(X_train)\n    loss = criterion(outputs, y_train)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Evaluation on the test set\nmodel.eval()\nwith torch.no_grad():\n    y_pred = model(X_test)\n    _, y_pred_labels = torch.max(y_pred, 1)\n    y_true_labels = y_test\n\n# Convert predictions back to original labels\ny_pred_original = label_encoder.inverse_transform(y_pred_labels.numpy())\ny_true_original = label_encoder.inverse_transform(y_true_labels.numpy())\n\n# Calculate accuracy and other metrics\naccuracy = accuracy_score(y_true_original, y_pred_original)\nreport = classification_report(y_true_original, y_pred_original)\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(\"Classification Report:\\n\", report)\n","metadata":{},"execution_count":19,"outputs":[{"name":"stdout","output_type":"stream","text":"Input size: torch.Size([838859, 0])\n\nMLPModel(\n\n  (fc1): Linear(in_features=0, out_features=64, bias=True)\n\n  (relu): ReLU()\n\n  (fc2): Linear(in_features=64, out_features=2, bias=True)\n\n)\n"},{"name":"stderr","output_type":"stream","text":"E:\\Conda\\Lib\\site-packages\\torch\\nn\\init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n\n  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"},{"name":"stdout","output_type":"stream","text":"Epoch [1/500], Loss: 0.6866\n\nEpoch [2/500], Loss: 0.6862\n\nEpoch [3/500], Loss: 0.6858\n\nEpoch [4/500], Loss: 0.6853\n\nEpoch [5/500], Loss: 0.6849\n\nEpoch [6/500], Loss: 0.6844\n\nEpoch [7/500], Loss: 0.6840\n\nEpoch [8/500], Loss: 0.6836\n\nEpoch [9/500], Loss: 0.6831\n\nEpoch [10/500], Loss: 0.6827\n\nEpoch [11/500], Loss: 0.6823\n\nEpoch [12/500], Loss: 0.6818\n\nEpoch [13/500], Loss: 0.6814\n\nEpoch [14/500], Loss: 0.6810\n\nEpoch [15/500], Loss: 0.6805\n\nEpoch [16/500], Loss: 0.6801\n\nEpoch [17/500], Loss: 0.6797\n\nEpoch [18/500], Loss: 0.6793\n\nEpoch [19/500], Loss: 0.6789\n\nEpoch [20/500], Loss: 0.6784\n\nEpoch [21/500], Loss: 0.6780\n\nEpoch [22/500], Loss: 0.6776\n\nEpoch [23/500], Loss: 0.6772\n\nEpoch [24/500], Loss: 0.6768\n\nEpoch [25/500], Loss: 0.6763\n\nEpoch [26/500], Loss: 0.6759\n\nEpoch [27/500], Loss: 0.6755\n\nEpoch [28/500], Loss: 0.6751\n\nEpoch [29/500], Loss: 0.6747\n\nEpoch [30/500], Loss: 0.6743\n\nEpoch [31/500], Loss: 0.6739\n\nEpoch [32/500], Loss: 0.6735\n\nEpoch [33/500], Loss: 0.6731\n\nEpoch [34/500], Loss: 0.6727\n\nEpoch [35/500], Loss: 0.6723\n\nEpoch [36/500], Loss: 0.6719\n\nEpoch [37/500], Loss: 0.6715\n\nEpoch [38/500], Loss: 0.6711\n\nEpoch [39/500], Loss: 0.6707\n\nEpoch [40/500], Loss: 0.6703\n\nEpoch [41/500], Loss: 0.6699\n\nEpoch [42/500], Loss: 0.6695\n\nEpoch [43/500], Loss: 0.6691\n\nEpoch [44/500], Loss: 0.6687\n\nEpoch [45/500], Loss: 0.6683\n\nEpoch [46/500], Loss: 0.6679\n\nEpoch [47/500], Loss: 0.6676\n\nEpoch [48/500], Loss: 0.6672\n\nEpoch [49/500], Loss: 0.6668\n\nEpoch [50/500], Loss: 0.6664\n\nEpoch [51/500], Loss: 0.6660\n\nEpoch [52/500], Loss: 0.6657\n\nEpoch [53/500], Loss: 0.6653\n\nEpoch [54/500], Loss: 0.6649\n\nEpoch [55/500], Loss: 0.6645\n\nEpoch [56/500], Loss: 0.6642\n\nEpoch [57/500], Loss: 0.6638\n\nEpoch [58/500], Loss: 0.6634\n\nEpoch [59/500], Loss: 0.6630\n\nEpoch [60/500], Loss: 0.6627\n\nEpoch [61/500], Loss: 0.6623\n\nEpoch [62/500], Loss: 0.6619\n\nEpoch [63/500], Loss: 0.6616\n\nEpoch [64/500], Loss: 0.6612\n\nEpoch [65/500], Loss: 0.6609\n\nEpoch [66/500], Loss: 0.6605\n\nEpoch [67/500], Loss: 0.6601\n\nEpoch [68/500], Loss: 0.6598\n\nEpoch [69/500], Loss: 0.6594\n\nEpoch [70/500], Loss: 0.6591\n\nEpoch [71/500], Loss: 0.6587\n\nEpoch [72/500], Loss: 0.6584\n\nEpoch [73/500], Loss: 0.6580\n\nEpoch [74/500], Loss: 0.6577\n\nEpoch [75/500], Loss: 0.6573\n\nEpoch [76/500], Loss: 0.6570\n\nEpoch [77/500], Loss: 0.6566\n\nEpoch [78/500], Loss: 0.6563\n\nEpoch [79/500], Loss: 0.6560\n\nEpoch [80/500], Loss: 0.6556\n\nEpoch [81/500], Loss: 0.6553\n\nEpoch [82/500], Loss: 0.6549\n\nEpoch [83/500], Loss: 0.6546\n\nEpoch [84/500], Loss: 0.6543\n\nEpoch [85/500], Loss: 0.6539\n\nEpoch [86/500], Loss: 0.6536\n\nEpoch [87/500], Loss: 0.6533\n\nEpoch [88/500], Loss: 0.6529\n\nEpoch [89/500], Loss: 0.6526\n\nEpoch [90/500], Loss: 0.6523\n\nEpoch [91/500], Loss: 0.6520\n\nEpoch [92/500], Loss: 0.6516\n\nEpoch [93/500], Loss: 0.6513\n\nEpoch [94/500], Loss: 0.6510\n\nEpoch [95/500], Loss: 0.6507\n\nEpoch [96/500], Loss: 0.6503\n\nEpoch [97/500], Loss: 0.6500\n\nEpoch [98/500], Loss: 0.6497\n\nEpoch [99/500], Loss: 0.6494\n\nEpoch [100/500], Loss: 0.6491\n\nEpoch [101/500], Loss: 0.6488\n\nEpoch [102/500], Loss: 0.6485\n\nEpoch [103/500], Loss: 0.6481\n\nEpoch [104/500], Loss: 0.6478\n\nEpoch [105/500], Loss: 0.6475\n\nEpoch [106/500], Loss: 0.6472\n\nEpoch [107/500], Loss: 0.6469\n\nEpoch [108/500], Loss: 0.6466\n\nEpoch [109/500], Loss: 0.6463\n\nEpoch [110/500], Loss: 0.6460\n\nEpoch [111/500], Loss: 0.6457\n\nEpoch [112/500], Loss: 0.6454\n\nEpoch [113/500], Loss: 0.6451\n\nEpoch [114/500], Loss: 0.6448\n\nEpoch [115/500], Loss: 0.6445\n\nEpoch [116/500], Loss: 0.6442\n\nEpoch [117/500], Loss: 0.6439\n\nEpoch [118/500], Loss: 0.6436\n\nEpoch [119/500], Loss: 0.6433\n\nEpoch [120/500], Loss: 0.6430\n\nEpoch [121/500], Loss: 0.6427\n\nEpoch [122/500], Loss: 0.6425\n\nEpoch [123/500], Loss: 0.6422\n\nEpoch [124/500], Loss: 0.6419\n\nEpoch [125/500], Loss: 0.6416\n\nEpoch [126/500], Loss: 0.6413\n\nEpoch [127/500], Loss: 0.6410\n\nEpoch [128/500], Loss: 0.6407\n\nEpoch [129/500], Loss: 0.6405\n\nEpoch [130/500], Loss: 0.6402\n\nEpoch [131/500], Loss: 0.6399\n\nEpoch [132/500], Loss: 0.6396\n\nEpoch [133/500], Loss: 0.6394\n\nEpoch [134/500], Loss: 0.6391\n\nEpoch [135/500], Loss: 0.6388\n\nEpoch [136/500], Loss: 0.6385\n\nEpoch [137/500], Loss: 0.6383\n\nEpoch [138/500], Loss: 0.6380\n\nEpoch [139/500], Loss: 0.6377\n\nEpoch [140/500], Loss: 0.6375\n\nEpoch [141/500], Loss: 0.6372\n\nEpoch [142/500], Loss: 0.6369\n\nEpoch [143/500], Loss: 0.6367\n\nEpoch [144/500], Loss: 0.6364\n\nEpoch [145/500], Loss: 0.6361\n\nEpoch [146/500], Loss: 0.6359\n\nEpoch [147/500], Loss: 0.6356\n\nEpoch [148/500], Loss: 0.6353\n\nEpoch [149/500], Loss: 0.6351\n\nEpoch [150/500], Loss: 0.6348\n\nEpoch [151/500], Loss: 0.6346\n\nEpoch [152/500], Loss: 0.6343\n\nEpoch [153/500], Loss: 0.6341\n\nEpoch [154/500], Loss: 0.6338\n\nEpoch [155/500], Loss: 0.6336\n\nEpoch [156/500], Loss: 0.6333\n\nEpoch [157/500], Loss: 0.6331\n\nEpoch [158/500], Loss: 0.6328\n\nEpoch [159/500], Loss: 0.6326\n\nEpoch [160/500], Loss: 0.6323\n\nEpoch [161/500], Loss: 0.6321\n\nEpoch [162/500], Loss: 0.6318\n\nEpoch [163/500], Loss: 0.6316\n\nEpoch [164/500], Loss: 0.6313\n\nEpoch [165/500], Loss: 0.6311\n\nEpoch [166/500], Loss: 0.6309\n\nEpoch [167/500], Loss: 0.6306\n\nEpoch [168/500], Loss: 0.6304\n\nEpoch [169/500], Loss: 0.6301\n\nEpoch [170/500], Loss: 0.6299\n\nEpoch [171/500], Loss: 0.6297\n\nEpoch [172/500], Loss: 0.6294\n\nEpoch [173/500], Loss: 0.6292\n\nEpoch [174/500], Loss: 0.6290\n\nEpoch [175/500], Loss: 0.6287\n\nEpoch [176/500], Loss: 0.6285\n\nEpoch [177/500], Loss: 0.6283\n\nEpoch [178/500], Loss: 0.6280\n\nEpoch [179/500], Loss: 0.6278\n\nEpoch [180/500], Loss: 0.6276\n\nEpoch [181/500], Loss: 0.6274\n\nEpoch [182/500], Loss: 0.6271\n\nEpoch [183/500], Loss: 0.6269\n\nEpoch [184/500], Loss: 0.6267\n\nEpoch [185/500], Loss: 0.6265\n\nEpoch [186/500], Loss: 0.6263\n\nEpoch [187/500], Loss: 0.6260\n\nEpoch [188/500], Loss: 0.6258\n\nEpoch [189/500], Loss: 0.6256\n\nEpoch [190/500], Loss: 0.6254\n\nEpoch [191/500], Loss: 0.6252\n\nEpoch [192/500], Loss: 0.6250\n\nEpoch [193/500], Loss: 0.6247\n\nEpoch [194/500], Loss: 0.6245\n\nEpoch [195/500], Loss: 0.6243\n\nEpoch [196/500], Loss: 0.6241\n\nEpoch [197/500], Loss: 0.6239\n\nEpoch [198/500], Loss: 0.6237\n\nEpoch [199/500], Loss: 0.6235\n\nEpoch [200/500], Loss: 0.6233\n\nEpoch [201/500], Loss: 0.6231\n\nEpoch [202/500], Loss: 0.6229\n\nEpoch [203/500], Loss: 0.6226\n\nEpoch [204/500], Loss: 0.6224\n\nEpoch [205/500], Loss: 0.6222\n\nEpoch [206/500], Loss: 0.6220\n\nEpoch [207/500], Loss: 0.6218\n\nEpoch [208/500], Loss: 0.6216\n\nEpoch [209/500], Loss: 0.6214\n\nEpoch [210/500], Loss: 0.6212\n\nEpoch [211/500], Loss: 0.6210\n\nEpoch [212/500], Loss: 0.6208\n\nEpoch [213/500], Loss: 0.6207\n\nEpoch [214/500], Loss: 0.6205\n\nEpoch [215/500], Loss: 0.6203\n\nEpoch [216/500], Loss: 0.6201\n\nEpoch [217/500], Loss: 0.6199\n\nEpoch [218/500], Loss: 0.6197\n\nEpoch [219/500], Loss: 0.6195\n\nEpoch [220/500], Loss: 0.6193\n\nEpoch [221/500], Loss: 0.6191\n\nEpoch [222/500], Loss: 0.6189\n\nEpoch [223/500], Loss: 0.6187\n\nEpoch [224/500], Loss: 0.6186\n\nEpoch [225/500], Loss: 0.6184\n\nEpoch [226/500], Loss: 0.6182\n\nEpoch [227/500], Loss: 0.6180\n\nEpoch [228/500], Loss: 0.6178\n\nEpoch [229/500], Loss: 0.6176\n\nEpoch [230/500], Loss: 0.6175\n\nEpoch [231/500], Loss: 0.6173\n\nEpoch [232/500], Loss: 0.6171\n\nEpoch [233/500], Loss: 0.6169\n\nEpoch [234/500], Loss: 0.6167\n\nEpoch [235/500], Loss: 0.6166\n\nEpoch [236/500], Loss: 0.6164\n\nEpoch [237/500], Loss: 0.6162\n\nEpoch [238/500], Loss: 0.6160\n\nEpoch [239/500], Loss: 0.6159\n\nEpoch [240/500], Loss: 0.6157\n\nEpoch [241/500], Loss: 0.6155\n\nEpoch [242/500], Loss: 0.6153\n\nEpoch [243/500], Loss: 0.6152\n\nEpoch [244/500], Loss: 0.6150\n\nEpoch [245/500], Loss: 0.6148\n\nEpoch [246/500], Loss: 0.6147\n\nEpoch [247/500], Loss: 0.6145\n\nEpoch [248/500], Loss: 0.6143\n\nEpoch [249/500], Loss: 0.6142\n\nEpoch [250/500], Loss: 0.6140\n\nEpoch [251/500], Loss: 0.6138\n\nEpoch [252/500], Loss: 0.6137\n\nEpoch [253/500], Loss: 0.6135\n\nEpoch [254/500], Loss: 0.6133\n\nEpoch [255/500], Loss: 0.6132\n\nEpoch [256/500], Loss: 0.6130\n\nEpoch [257/500], Loss: 0.6129\n\nEpoch [258/500], Loss: 0.6127\n\nEpoch [259/500], Loss: 0.6125\n\nEpoch [260/500], Loss: 0.6124\n\nEpoch [261/500], Loss: 0.6122\n\nEpoch [262/500], Loss: 0.6121\n\nEpoch [263/500], Loss: 0.6119\n\nEpoch [264/500], Loss: 0.6118\n\nEpoch [265/500], Loss: 0.6116\n\nEpoch [266/500], Loss: 0.6115\n\nEpoch [267/500], Loss: 0.6113\n\nEpoch [268/500], Loss: 0.6112\n\nEpoch [269/500], Loss: 0.6110\n\nEpoch [270/500], Loss: 0.6109\n\nEpoch [271/500], Loss: 0.6107\n\nEpoch [272/500], Loss: 0.6106\n\nEpoch [273/500], Loss: 0.6104\n\nEpoch [274/500], Loss: 0.6103\n\nEpoch [275/500], Loss: 0.6101\n\nEpoch [276/500], Loss: 0.6100\n\nEpoch [277/500], Loss: 0.6098\nEpoch [278/500], Loss: 0.6097\n\nEpoch [279/500], Loss: 0.6095\n\nEpoch [280/500], Loss: 0.6094\n\nEpoch [281/500], Loss: 0.6093\n\nEpoch [282/500], Loss: 0.6091\n\nEpoch [283/500], Loss: 0.6090\n\nEpoch [284/500], Loss: 0.6088\n\nEpoch [285/500], Loss: 0.6087\n\nEpoch [286/500], Loss: 0.6086\n\nEpoch [287/500], Loss: 0.6084\n\nEpoch [288/500], Loss: 0.6083\n\nEpoch [289/500], Loss: 0.6081\n\nEpoch [290/500], Loss: 0.6080\n\nEpoch [291/500], Loss: 0.6079\n\nEpoch [292/500], Loss: 0.6077\n\nEpoch [293/500], Loss: 0.6076\n\nEpoch [294/500], Loss: 0.6075\n\nEpoch [295/500], Loss: 0.6073\n\nEpoch [296/500], Loss: 0.6072\n\nEpoch [297/500], Loss: 0.6071\n\nEpoch [298/500], Loss: 0.6069\n\nEpoch [299/500], Loss: 0.6068\n\nEpoch [300/500], Loss: 0.6067\n\nEpoch [301/500], Loss: 0.6065\n\nEpoch [302/500], Loss: 0.6064\n\nEpoch [303/500], Loss: 0.6063\n\nEpoch [304/500], Loss: 0.6062\n\nEpoch [305/500], Loss: 0.6060\n\nEpoch [306/500], Loss: 0.6059\n\nEpoch [307/500], Loss: 0.6058\n\nEpoch [308/500], Loss: 0.6057\n\nEpoch [309/500], Loss: 0.6055\n\nEpoch [310/500], Loss: 0.6054\n\nEpoch [311/500], Loss: 0.6053\n\nEpoch [312/500], Loss: 0.6052\n\nEpoch [313/500], Loss: 0.6050\n\nEpoch [314/500], Loss: 0.6049\n\nEpoch [315/500], Loss: 0.6048\n\nEpoch [316/500], Loss: 0.6047\n\nEpoch [317/500], Loss: 0.6046\n\nEpoch [318/500], Loss: 0.6044\n\nEpoch [319/500], Loss: 0.6043\n\nEpoch [320/500], Loss: 0.6042\n\nEpoch [321/500], Loss: 0.6041\n\nEpoch [322/500], Loss: 0.6040\n\nEpoch [323/500], Loss: 0.6039\n\nEpoch [324/500], Loss: 0.6038\n\nEpoch [325/500], Loss: 0.6036\n\nEpoch [326/500], Loss: 0.6035\n\nEpoch [327/500], Loss: 0.6034\n\nEpoch [328/500], Loss: 0.6033\n\nEpoch [329/500], Loss: 0.6032\n\nEpoch [330/500], Loss: 0.6031\n\nEpoch [331/500], Loss: 0.6030\n\nEpoch [332/500], Loss: 0.6029\n\nEpoch [333/500], Loss: 0.6027\n\nEpoch [334/500], Loss: 0.6026\n\nEpoch [335/500], Loss: 0.6025\n\nEpoch [336/500], Loss: 0.6024\n\nEpoch [337/500], Loss: 0.6023\n\nEpoch [338/500], Loss: 0.6022\n\nEpoch [339/500], Loss: 0.6021\n\nEpoch [340/500], Loss: 0.6020\n\nEpoch [341/500], Loss: 0.6019\n\nEpoch [342/500], Loss: 0.6018\n\nEpoch [343/500], Loss: 0.6017\n\nEpoch [344/500], Loss: 0.6016\n\nEpoch [345/500], Loss: 0.6015\n\nEpoch [346/500], Loss: 0.6014\n\nEpoch [347/500], Loss: 0.6013\n\nEpoch [348/500], Loss: 0.6012\n\nEpoch [349/500], Loss: 0.6011\n\nEpoch [350/500], Loss: 0.6010\n\nEpoch [351/500], Loss: 0.6009\n\nEpoch [352/500], Loss: 0.6008\n\nEpoch [353/500], Loss: 0.6007\n\nEpoch [354/500], Loss: 0.6006\n\nEpoch [355/500], Loss: 0.6005\n\nEpoch [356/500], Loss: 0.6004\n\nEpoch [357/500], Loss: 0.6003\n\nEpoch [358/500], Loss: 0.6002\n\nEpoch [359/500], Loss: 0.6001\n\nEpoch [360/500], Loss: 0.6000\n\nEpoch [361/500], Loss: 0.5999\n\nEpoch [362/500], Loss: 0.5998\n\nEpoch [363/500], Loss: 0.5997\n\nEpoch [364/500], Loss: 0.5996\n\nEpoch [365/500], Loss: 0.5995\n\nEpoch [366/500], Loss: 0.5994\n\nEpoch [367/500], Loss: 0.5993\n\nEpoch [368/500], Loss: 0.5992\n\nEpoch [369/500], Loss: 0.5992\n\nEpoch [370/500], Loss: 0.5991\n\nEpoch [371/500], Loss: 0.5990\n\nEpoch [372/500], Loss: 0.5989\n\nEpoch [373/500], Loss: 0.5988\n\nEpoch [374/500], Loss: 0.5987\n\nEpoch [375/500], Loss: 0.5986\n\nEpoch [376/500], Loss: 0.5985\n\nEpoch [377/500], Loss: 0.5984\n\nEpoch [378/500], Loss: 0.5984\n\nEpoch [379/500], Loss: 0.5983\n\nEpoch [380/500], Loss: 0.5982\n\nEpoch [381/500], Loss: 0.5981\n\nEpoch [382/500], Loss: 0.5980\n\nEpoch [383/500], Loss: 0.5979\n\nEpoch [384/500], Loss: 0.5978\n\nEpoch [385/500], Loss: 0.5978\n\nEpoch [386/500], Loss: 0.5977\n\nEpoch [387/500], Loss: 0.5976\n\nEpoch [388/500], Loss: 0.5975\n\nEpoch [389/500], Loss: 0.5974\n\nEpoch [390/500], Loss: 0.5974\n\nEpoch [391/500], Loss: 0.5973\n\nEpoch [392/500], Loss: 0.5972\n\nEpoch [393/500], Loss: 0.5971\n\nEpoch [394/500], Loss: 0.5970\n\nEpoch [395/500], Loss: 0.5970\n\nEpoch [396/500], Loss: 0.5969\n\nEpoch [397/500], Loss: 0.5968\n\nEpoch [398/500], Loss: 0.5967\n\nEpoch [399/500], Loss: 0.5966\n\nEpoch [400/500], Loss: 0.5966\n\nEpoch [401/500], Loss: 0.5965\n\nEpoch [402/500], Loss: 0.5964\n\nEpoch [403/500], Loss: 0.5963\n\nEpoch [404/500], Loss: 0.5963\n\nEpoch [405/500], Loss: 0.5962\n\nEpoch [406/500], Loss: 0.5961\n\nEpoch [407/500], Loss: 0.5960\n\nEpoch [408/500], Loss: 0.5960\n\nEpoch [409/500], Loss: 0.5959\n\nEpoch [410/500], Loss: 0.5958\n\nEpoch [411/500], Loss: 0.5957\n\nEpoch [412/500], Loss: 0.5957\n\nEpoch [413/500], Loss: 0.5956\n\nEpoch [414/500], Loss: 0.5955\n\nEpoch [415/500], Loss: 0.5955\n\nEpoch [416/500], Loss: 0.5954\n\nEpoch [417/500], Loss: 0.5953\n\nEpoch [418/500], Loss: 0.5953\n\nEpoch [419/500], Loss: 0.5952\n\nEpoch [420/500], Loss: 0.5951\n\nEpoch [421/500], Loss: 0.5950\n\nEpoch [422/500], Loss: 0.5950\n\nEpoch [423/500], Loss: 0.5949\n\nEpoch [424/500], Loss: 0.5948\n\nEpoch [425/500], Loss: 0.5948\n\nEpoch [426/500], Loss: 0.5947\n\nEpoch [427/500], Loss: 0.5946\n\nEpoch [428/500], Loss: 0.5946\n\nEpoch [429/500], Loss: 0.5945\n\nEpoch [430/500], Loss: 0.5945\n\nEpoch [431/500], Loss: 0.5944\n\nEpoch [432/500], Loss: 0.5943\n\nEpoch [433/500], Loss: 0.5943\n\nEpoch [434/500], Loss: 0.5942\n\nEpoch [435/500], Loss: 0.5941\n\nEpoch [436/500], Loss: 0.5941\n\nEpoch [437/500], Loss: 0.5940\n\nEpoch [438/500], Loss: 0.5939\n\nEpoch [439/500], Loss: 0.5939\n\nEpoch [440/500], Loss: 0.5938\n\nEpoch [441/500], Loss: 0.5938\n\nEpoch [442/500], Loss: 0.5937\n\nEpoch [443/500], Loss: 0.5936\n\nEpoch [444/500], Loss: 0.5936\n\nEpoch [445/500], Loss: 0.5935\n\nEpoch [446/500], Loss: 0.5935\n\nEpoch [447/500], Loss: 0.5934\n\nEpoch [448/500], Loss: 0.5933\n\nEpoch [449/500], Loss: 0.5933\n\nEpoch [450/500], Loss: 0.5932\n\nEpoch [451/500], Loss: 0.5932\n\nEpoch [452/500], Loss: 0.5931\n\nEpoch [453/500], Loss: 0.5931\n\nEpoch [454/500], Loss: 0.5930\n\nEpoch [455/500], Loss: 0.5929\n\nEpoch [456/500], Loss: 0.5929\n\nEpoch [457/500], Loss: 0.5928\n\nEpoch [458/500], Loss: 0.5928\n\nEpoch [459/500], Loss: 0.5927\n\nEpoch [460/500], Loss: 0.5927\n\nEpoch [461/500], Loss: 0.5926\n\nEpoch [462/500], Loss: 0.5926\n\nEpoch [463/500], Loss: 0.5925\n\nEpoch [464/500], Loss: 0.5925\n\nEpoch [465/500], Loss: 0.5924\n\nEpoch [466/500], Loss: 0.5924\n\nEpoch [467/500], Loss: 0.5923\n\nEpoch [468/500], Loss: 0.5922\n\nEpoch [469/500], Loss: 0.5922\n\nEpoch [470/500], Loss: 0.5921\n\nEpoch [471/500], Loss: 0.5921\n\nEpoch [472/500], Loss: 0.5920\n\nEpoch [473/500], Loss: 0.5920\n\nEpoch [474/500], Loss: 0.5919\n\nEpoch [475/500], Loss: 0.5919\n\nEpoch [476/500], Loss: 0.5918\n\nEpoch [477/500], Loss: 0.5918\n\nEpoch [478/500], Loss: 0.5917\n\nEpoch [479/500], Loss: 0.5917\n\nEpoch [480/500], Loss: 0.5916\n\nEpoch [481/500], Loss: 0.5916\n\nEpoch [482/500], Loss: 0.5916\n\nEpoch [483/500], Loss: 0.5915\n\nEpoch [484/500], Loss: 0.5915\n\nEpoch [485/500], Loss: 0.5914\n\nEpoch [486/500], Loss: 0.5914\n\nEpoch [487/500], Loss: 0.5913\n\nEpoch [488/500], Loss: 0.5913\n\nEpoch [489/500], Loss: 0.5912\n\nEpoch [490/500], Loss: 0.5912\n\nEpoch [491/500], Loss: 0.5911\n\nEpoch [492/500], Loss: 0.5911\n\nEpoch [493/500], Loss: 0.5910\n\nEpoch [494/500], Loss: 0.5910\n\nEpoch [495/500], Loss: 0.5910\n\nEpoch [496/500], Loss: 0.5909\n\nEpoch [497/500], Loss: 0.5909\n\nEpoch [498/500], Loss: 0.5908\n\nEpoch [499/500], Loss: 0.5908\n\nEpoch [500/500], Loss: 0.5907\n\nAccuracy: 0.73\n\nClassification Report:\n\n               precision    recall  f1-score   support\n\n\n\n           0       0.00      0.00      0.00     57170\n\n           1       0.73      1.00      0.84    152545\n\n\n\n    accuracy                           0.73    209715\n\n   macro avg       0.36      0.50      0.42    209715\n\nweighted avg       0.53      0.73      0.61    209715\n\n\n"},{"name":"stderr","output_type":"stream","text":"E:\\Conda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n\n  _warn_prf(average, modifier, msg_start, len(result))\n\nE:\\Conda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n\n  _warn_prf(average, modifier, msg_start, len(result))\n\nE:\\Conda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n\n  _warn_prf(average, modifier, msg_start, len(result))\n"}]},{"cell_type":"code","source":"data.head()","metadata":{},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Type</th>\n","      <th>Cycle</th>\n","      <th>Time</th>\n","      <th>RR-I</th>\n","      <th>HR</th>\n","      <th>R-H</th>\n","      <th>P-H</th>\n","      <th>QRS</th>\n","      <th>PRQ</th>\n","      <th>QT</th>\n","      <th>QTC</th>\n","      <th>ST</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1.072</td>\n","      <td>0.932</td>\n","      <td>64.377682</td>\n","      <td>3.305</td>\n","      <td>3.81</td>\n","      <td>0.088</td>\n","      <td>0.16</td>\n","      <td>0.384</td>\n","      <td>0.397762</td>\n","      <td>0.296</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2.004</td>\n","      <td>1.004</td>\n","      <td>59.760956</td>\n","      <td>3.285</td>\n","      <td>3.81</td>\n","      <td>0.1</td>\n","      <td>0.16</td>\n","      <td>0.64</td>\n","      <td>0.638724</td>\n","      <td>0.54</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>3.008</td>\n","      <td>0.996</td>\n","      <td>60.240964</td>\n","      <td>3.34</td>\n","      <td>3.83</td>\n","      <td>0.128</td>\n","      <td>0.148</td>\n","      <td>0.628</td>\n","      <td>0.62926</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>4.004</td>\n","      <td>1.016</td>\n","      <td>59.055118</td>\n","      <td>3.325</td>\n","      <td>3.81</td>\n","      <td>0.108</td>\n","      <td>0.172</td>\n","      <td>0.388</td>\n","      <td>0.384933</td>\n","      <td>0.28</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>5.02</td>\n","      <td>0.972</td>\n","      <td>61.728395</td>\n","      <td>3.29</td>\n","      <td>3.765</td>\n","      <td>0.1</td>\n","      <td>0.16</td>\n","      <td>0.372</td>\n","      <td>0.37732</td>\n","      <td>0.272</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["0  Type Cycle   Time   RR-I         HR    R-H    P-H    QRS    PRQ     QT  \\\n","1     1     1  1.072  0.932  64.377682  3.305   3.81  0.088   0.16  0.384   \n","2     1     2  2.004  1.004  59.760956  3.285   3.81    0.1   0.16   0.64   \n","3     1     3  3.008  0.996  60.240964   3.34   3.83  0.128  0.148  0.628   \n","4     1     4  4.004  1.016  59.055118  3.325   3.81  0.108  0.172  0.388   \n","5     1     5   5.02  0.972  61.728395   3.29  3.765    0.1   0.16  0.372   \n","\n","0       QTC     ST  \n","1  0.397762  0.296  \n","2  0.638724   0.54  \n","3   0.62926    0.5  \n","4  0.384933   0.28  \n","5   0.37732  0.272  "]},"metadata":{}}]},{"cell_type":"code","source":"%time \nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\nfrom keras.utils import to_categorical\n\n# Assuming your DataFrame is named 'data'\n# Assuming 'Type' is the column with the class labels\n\n# Drop non-numeric columns or encode them if needed\ndata_numeric = data.select_dtypes(include=['number'])\n\n# Create feature matrix X and target variable y\nX = data_numeric.drop('Type', axis=1)\ny = data_numeric['Type']\n\n# Convert labels to numerical values using LabelEncoder\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Convert numerical labels to one-hot encoding\ny_one_hot = to_categorical(y_encoded)\n\n# Set the maximum window size based on available data\nmax_window_size = len(X)\n\n# Decrease the window size until it fits the data\ninput_window_size = max_window_size\nwhile input_window_size > 1:\n    X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n    \n    X_train_reshaped = np.array([X_train.iloc[i:i+input_window_size].values for i in range(len(X_train)-input_window_size+1)])\n    X_test_reshaped = np.array([X_test.iloc[i:i+input_window_size].values for i in range(len(X_test)-input_window_size+1)])\n    \n    if X_train_reshaped.size > 0 and X_test_reshaped.size > 0:\n        break\n    \n    input_window_size -= 1\nprint(\"Printed\")\n\n# Check if the adjusted window size is suitable\nif input_window_size <= 1:\n    raise ValueError(\"No suitable window size found. Adjust the window size or provide more data.\")\n\n# Print the adjusted window size\nprint(f\"Adjusted input window size: {input_window_size}\")\n\n# Reshape the input data to have one channel\nX_train_reshaped = X_train_reshaped.reshape((X_train_reshaped.shape[0], X_train_reshaped.shape[1], 1))\nX_test_reshaped = X_test_reshaped.reshape((X_test_reshaped.shape[0], X_test_reshaped.shape[1], 1))\n\n# Build the CNN model\nmodel = Sequential()\nmodel.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(len(label_encoder.classes_), activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nimport time\n\n# ... (your existing code)\n\n# Record the start time\nstart_time = time.time()\n\n# Train the model\nprint(\"Printed\")\nmodel.fit(X_train_reshaped, y_train, epochs=2, batch_size=32, validation_split=0.2)\nend_time = time.time()\n\n# Calculate the time taken\ntraining_time = end_time - start_time\nprint(f\"Training time: {training_time} seconds\")\n\n# Evaluate the model on the test set\ny_pred = model.predict(X_test_reshaped)\ny_pred_labels = np.argmax(y_pred, axis=1)\ny_true_labels = np.argmax(y_test, axis=1)\n\n# Convert predictions back to original labels\ny_pred_original = label_encoder.inverse_transform(y_pred_labels)\ny_true_original = label_encoder.inverse_transform(y_true_labels)\n\n# Calculate accuracy and other metrics\naccuracy = accuracy_score(y_true_original, y_pred_original)\nreport = classification_report(y_true_original, y_pred_original)\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(\"Classification Report:\\n\", report)\n","metadata":{},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"CPU times: total: 0 ns\n\nWall time: 0 ns\n"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}